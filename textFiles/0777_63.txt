EXECUTIVE SUMMARY 
 
This project is related to music and machine learning. In the previous project they use 
the self-similarity matrix to analyze the musical form. We start with their work, and 
extend it by adding more theoretical topics. We talk about spectral algorithm and 
dimensionality reduction, and their relationship to SVD. We use SVD to decompose 
the similarity matrix of music and analyze the meaning of it. By doing some 
experiments, we research the performance of our method and discuss the reason of its 
advantage. We also study some important kinds of SVD implementation algorithms, 
and compare them by designed experiments. Our software implementation can be 
seen as an updated version of the software in previous project by adding SVD analysis 
functions and some more applications. 
 
? I researched and discussed some interesting topics in spectral algorithm and 
dimensionality reduction, include relationship between PCA and SVD and LSI, 
Laplacian Eigenmap and related problems, etc. 
 
? I successfully used SVD in musical structure discovery. This topic has been 
researched by few researchers and my method performs very well for some 
specific kinds of music 
 
? I implemented an updated version of the software, added SVD analysis in it and 
some more functions are implemented. 
 
? I compare some popular algorithms of calculating SVD, see page 52-59 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
ACKNOWLEDGEMENTS 
Firstly I would like to thank my supervisor Perter Flach. He is always very serious to 
my project and his guidance helps me solve many problems. He makes me know that 
math can also be as interesting as art. 
 
And I have to thank my friends, who help me treat printing things when I am out of 
UK. 
 
Finally, thanks to my parents, they are always kind to me and support me. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
TABLE OF CONTENTS 
 
1. INTRODUCTION ......................................................................................................................... 1 
1.1 AIMS AND OBJECTIVES .......................................................................................................... 1 
1.1.1 Theoretical Analysis Objectives ......................................................................................... 1 
1.1.2 Software Implementation Objectives ................................................................................. 1 
1.2 STRUCTURE OF THESIS .......................................................................................................... 2 
2. MUSICAL BACKGROUND ........................................................................................................ 3 
2.1 BASIC TERMS .......................................................................................................................... 4 
2.2 HARMONIC ANALYSIS ............................................................................................................ 6 
2.3 MUSICAL FORM ...................................................................................................................... 7 
2.4 MIDI ....................................................................................................................................... 8 
2.5 OTHERS ................................................................................................................................ 10 
3 THEORETICAL BACKGROUND ............................................................................................ 11 
3.1 DIMENSIONALITY REDUCTION ............................................................................................ 11 
3.1.1 Introduction ..................................................................................................................... 11 
3.1.2 Feature ............................................................................................................................. 11 
3.1.3 Eigenvalue ....................................................................................................................... 12 
3.1.4 Vector Quantization (VQ) ................................................................................................ 13 
3.1.5 Linear discriminant analysis (LDA) ............................................................................... 13 
3.1.6 Principal component analysis (PCA) .............................................................................. 16 
3.1.7 Singular value decomposition (SVD) .............................................................................. 18 
3.1.8 Summary .......................................................................................................................... 18 
3.2 SPECTRAL CLUSTERING ....................................................................................................... 19 
3.2.1 Introduction ..................................................................................................................... 19 
3.2.2 Laplacian Eigenmaps ...................................................................................................... 20 
3.2.3 Graph Cut ........................................................................................................................ 24 
3.2.4 Other Topics ..................................................................................................................... 27 
3.2.4.1 Minimum conductance ........................................................................................................... 27 
3.2.4.2 Reformulate the classical clustering methods ........................................................................ 28 
3.2.4.3 Others ..................................................................................................................................... 28 
4 RELEVANT WORKS ................................................................................................................. 29 
4.1 THE PREVIOUS PROJECT ..................................................................................................... 29 
4.2 MUSIC GENERATION ............................................................................................................ 29 
4.3 MUSIC STRUCTURE ANALYSIS .............................................................................................. 29 
5 METHOD ..................................................................................................................................... 30 
5.1 INTRODUCTION ..................................................................................................................... 30 
5.2 MODELING ............................................................................................................................ 31 
5.3 BUILDING THE MATRIX ......................................................................................................... 33 
5.4 CLUSTERING ......................................................................................................................... 34 
 
 
5.4.1 Segmentation ................................................................................................................... 34 
5.4.2 Application of SVD .......................................................................................................... 35 
5.4.3 Example Experiment ....................................................................................................... 38 
5.4.4 Software Implemetation ................................................................................................... 40 
5.4.5 More Applications ............................................................................................................ 42 
5.4.5.1 Musical Retrieval ................................................................................................................... 42 
5.4.5.2 Automatic Summarization...................................................................................................... 43 
5.4.5.3 Audio Segmentation ............................................................................................................... 43 
5.4.5.4 Others ..................................................................................................................................... 43 
6 RESULT ........................................................................................................................................ 44 
6.1 CLUSTERING EXPERIMENTS ................................................................................................ 44 
6.2 DISTANCE FUNCTION SELECTION ........................................................................................ 47 
7 DISCUSSION ............................................................................................................................... 48 
7.1 MEANING OF SVD ................................................................................................................ 48 
7.2 SVD ANALYSIS ..................................................................................................................... 49 
7.2.1 Introduction ..................................................................................................................... 49 
7.2.2 The application of SVD ................................................................................................... 51 
7.2.3 SVD Algorithm ................................................................................................................. 53 
7.2.4 Method comparison ......................................................................................................... 60 
8 CONCLUSION ............................................................................................................................ 61 
8.1 EVALUATION ......................................................................................................................... 61 
8.2 FURTHER WORK................................................................................................................... 62 
HMM .............................................................................................................................................. 62 
Spectral Algorithm and manifold ................................................................................................... 62 
Other Similarity Matrix ................................................................................................................. 62 
REFERENCE ....................................................................................................................................... 63 
APPENDICES ...................................................................................................................................... 67 
APPENDIX A: LAPLACIAN EIGENMAP .................................................................................................. 67 
 
 
 
 
 
 
 
 
 
 
 
 
1. INTRODUCTION 
1 
 
1. INTRODUCTION 
This section includes a statement of the aims and objectives of the project and then I 
explain the structure of this thesis. 
 
1.1  Aims and Objectives 
 
The project is related to music. The primary aim of this project is to research the 
methods of matrix decompositions and come up with a general method of structure 
discovery in music using matrix decompositions. And this includes two aspects, the 
theoretical analysis of the general method and the software implementation as an 
application of musical structure analysis. 
 
1.1.1 Theoretical Analysis Objectives 
 
a) Compare different methods of matrix decomposition. Discuss their 
advantages and disadvantages. 
b) Research the mathematical meaning of SVD, include its meaning and 
benefit for matrix decomposition. 
c) Study the different algorithm of SVD, compare their performances and 
complexity. Discuss the constrain conditions of them. 
d) Find out how about the performance of SVD when it is used in musical 
structure discovery. 
e) Study how to model the bar and build the self-similarity matrix. 
f) Research the matrix selection to discover what will happen if we build 
different kinds of similarity matrix instead of self-similarity matrix. 
g) Compare the known methods of musical structure analysis. 
h) Discuss some other applications of the result of SVD related to musical 
structure. 
 
1.1.2 Software Implementation Objectives 
 
a) Implement the algorithm of SVD. 
b) The software can model the bars and build the matrix of MIDI music. 
c) It use SVD automatically analyze the structure. 
d) Implement the Graphical User Interface. It can accept MIDI file as input 
and output a detail report of the music and a graphical matrix. 
e) Users can set the thresholds and resize the graphical matrix or window. 
They can also change the colors. 
f) Users can get some temporary data include numerical data and some 
graphs like heat map. 
g) The software should be fast and stable. It should have good frame in the 
source code since this software is designed as a general system of musical 
structure analysis. 
 
2 
 
1.2  Structure of Thesis 
 
Firstly we state the aims and objectives of this project include theoretical aims and 
software aims in Chapter 1. And we briefly introduce the structure of the thesis. 
 
Since this project is about music, we should first study the fundamental musical 
background. In Chapter 2 we introduce the basic knowledge of music theory, 
especially the musical form knowledge. And we represent music as MIDI files in this 
project.  
 
Then we consider how to connect music and math In Chapter 3 we introduce the 
theoretical background includes spectral algorithms and dimensionality reduction 
methods. We not only describe the definition and conditions of them, we also discuss 
the mathematical meaning of them and how they are related to musical structure 
discovery. And we use derivations to prove their advantages. 
 
In Chapter 4 we survey some relevant works include music generation, the previous 
project [4] and musical structure analysis. Among this works, the previous project [4] 
is the most important one, since our project is based on the topics and software 
framework in this project and extent to SVD analysis. 
 
In Chapter 5 we describe the details of how we design the experiments, include vector 
modeling, matrix building and SVD application. We introduce the basic experiment 
framework in our project and give a simple example. We also briefly introduce the 
software implementation which is an updated version of the previous one. 
 
In Chapter 6 we analyze the experiment result. We discuss the parameter selection and 
model selection, which include how to choose the matrix and distance function. We 
compare the performance between our method and the other methods include the one 
used in the previous project [4]. 
 
In Chapter 7 we focus on SVD itself. We firstly research the mathematical meaning of 
SVD, and then introduce the definition and properties of it. We also list some 
applications of SVD except musical structure analysis. Then we detailed the 
algorithms of SVD. We give some important algorithm frameworks of it and briefly 
compare their performance. 
 
In Chapter 8 we state the evaluation and further work. This is a short chapter since 
there are no too many technical topics and experiments. 
 
 
 
 
 
 2. 
 
Sinc
sect
stru
grap
MUSICAL
e this proj
ion include
cture know
h we can o
 BACKGR
ect is based
s some co
ledge. Ther
verview the
Figure
OUND 
 on music,
ncepts and
e is a conce
 concept le
 1 The fram
 I will first
 notations 
ptual mod
vels. 
 
 
 
 
ework of m
 
 
 
 
 
 
 
 
 
 introduce 
of music, 
el of music
usic struc
basic music
and some r
structure b
ture [1] 
al theory. 
elated mu
elow; from
3 
This 
sical 
 this 
 2.1 
 
i.
ii.
iii.
iv.
v.
vi.
vii.
Basic Term
 Pitch: H
the soun
 Note: A
 Scale: A
 
 Interval
Whole, 
 
 
 Beat: B
 Loudne
 Rhythm
 
 
s 
ow human
d. A kind o
 notation of
 sequence 
Figure 2
 (Duration)
Half, Quar
Figu
asic units re
ss: How lou
: the patter
Figure 4 An
 feel the fr
f perceptua
 pitch of a 
of notes. 
 Successio
: Distance 
ter, Eighth, 
re 3 A hiera
present pu
d human h
n of duratio
 example o
equency o
l analog of
sound. 
n of music 
between tw
sixteenth, e
rchical repr
lse. 
ear. A kind
ns 
f rhythm: C
f sound, ho
 frequency.
notes and m
o notes, h
tc. 
esentation 
of perceptu
ompound 
w high or 
 
usic Scale
ow long th
 
of Duration
al analog o
 
triple drum
how low a
 [2, 3] 
ey last. Inc
s 
f amplitude
 pattern 
4 
bout 
 
lude 
. 
  
viii.
ix.
x.
xi.
Fig
 
2.2 
 
Mus
righ
and 
 
 Bar (M
pattern 
 
 Meter: t
 Chord: 
 Harmon
 
ure 6 An e
Harmonic 
ic is an ar
t chord wh
mark the ch
easure): A s
of recurring
ype of mea
A set of not
y: Combin
xample of i
Analysis 
t form whi
ich fit a giv
ord symbo
egment of 
 beats. 
Figure
sure, writte
es 
ation of cho
mplied harm
Suite no
ch exhibit 
en piece of
ls beside th
time in a du
 5 Three ex
n as fractio
rds or som
onies in a
. 1 in G, BW
 
rich structu
 notes with
e notes if n
ration of g
ample Bars
n like 4/4, 
e notes sou
 monodic li
V 1007 
re. Harmo
out giving 
ecessary.
iven numbe
 [1] 
2/2, etc. 
nd in the sa
ne. From J.
nic analysi
any harmon
r of beats. 
me time 
S. Bach Ce
s is to find
ic informa
5 
The 
 
llo 
 the 
tion, 
 
  
Har
and 
mat
harm
syst
 
 
2.3 
 
Mus
patt
som
end
do n
are 
 
Nam
Ron
Bin
Rou
Bar 
Tern
The
Son
Son
 
Figure 7 A
monic anal
find the fi
hematics n
onic analy
em and neu
Musical Fo
ical form i
erns which
e different 
ing. Introdu
ot consider
in this form
e  
do 
ary 
nded Binar
Form 
ary 
me and Var
g Form 
ata 
n example 
ysis is usua
ttest one. I
ow, and rel
sis has mo
roscience. 
rm 
s one of the
 often occu
kinds of m
ction and e
 them. A si
. And this i
S
A
A
y A
A
A
iations A
A
E
ca
Tab
harmonic a
lly a comp
t is a kind
ated to Fou
re applicat
 most impo
r in music
usic pieces
nding often
mple exam
s actually “
ymbolic Re
 B A C A D
 B 
 B A 
 A B 
 B A' 
 A' A'' 
 A B A 
xposition-D
pitulation
le 1 Some 
nalysis of “
lex job. We
 of searchi
rier series 
ions in som
rtant aspec
. It describe
; they are in
 appear jus
ple is just v
Binary For
presentati
 A 
evelopmen
common m
Minuet of 
 need com
ng of bars
and Fourie
e other su
ts of music 
s music st
troduction
t one or ze
erse-choru
m”, can be 
on Not
A co
play
sect
 
Only
time
 
 
 
B is
t-Re Incl
Rom
usical form
Forest” from
pare all the
. It is even
r transform
bjects like 
structure. S
ructure of t
, verse, cho
ro time in o
s [4]. Most s
written as A
es 
mmon retu
ed between
ions. 
 part of A 
. 
often called
ude Baroqu
antic 
s [4] 
 <Zelda> 
 known ch
 the branc
s. Interesti
physics, si
imply, they
ime. There
rus, bridge
ne song, so
hort pop so
B.  
rning them
 contrasting
played seco
 the ‘bridg
e, Classica
6 
 
ords 
h of 
ngly 
gnal 
 are 
 are 
 and 
 we 
ngs 
e 
 
nd 
e’. 
l, 
  
For 
eigh
and 
can 
ther
 
In th
com
 
 
2.4 
 
MID
prot
 
To b
MID
the 
mus
 
The
the 
soun
need
base
we 
disc
 
popular son
th, INST-in
content, we
find the cor
e is an exam
is project w
mon music
Figu
MIDI 
I is the sh
ocol and fir
e more gen
I (Musical
audio wav
ical forms.
 MIDI file j
distortions.
d. MIDI f
 in the pro
d informat
will use the
uss how to 
gs, their st
strumental
 can get th
responding
ple. 
e will mai
al forms fo
re 8 An exa
ort of Musi
stly for syn
eral and co
 Instrumen
e signal an
 
ust include
 And we c
iles include
ject. But M
ion since th
 MIDI read
exact infor
ructure con
 sections an
e clusters. T
 different p
nly focus o
r classic mu
mple of clu
cal Instrum
thesizers to
mpatible w
t Digital In
d the noise
s all the ev
an also bui
 notes, tim
IDI files d
ere are onl
er compon
mation and
tains Intro, 
d Outro. B
here are m
atterns for 
n classic m
sic. 
stering bas
ent Digital
 communic
ith the pre
terface) file
s. We can 
ents, not th
ld some tes
es, scales, 
o not inclu
y instructio
ent from th
 how we wi
Verse, Cho
ased on the
any rules in
different so
usic. In Tab
ed on sema
 Interface, 
ate/control
vious proje
s as input. 
just focus 
e waves. So
t files by t
tempo a
de any sig
ns for real
e previous
ll process a
rus, Bridge
 similarity o
 Musical fo
ngs. In the 
le 1 there a
ntic similar
it is Develo
 each other
ct [4], we u
So we will
on the mu
 we need n
yping, need
nd all the 
nal inform
or virtual sy
 project [4]
nd use them
, Middle 
f the melo
rm, and w
figure belo
re some 
ity [1] 
ped in 198
 
se the stan
 never cons
sical notes 
ot worry a
 not make
information
ation and w
nthesizer. 
. Later we 
. 
7 
dy 
e 
w 
 
3 as 
dard 
ider 
and 
bout 
 any 
 we 
ave 
And 
will 
 In th
mos
SMF
 
e SMF (St
t frequently
 file: 
Figure 9 M
andard MI
 used even
Figure 
IDI music g
DI Files) fi
ts are “not
10 An exam
enerating p
 
 
les, pitch i
e on” and “
ple of SMF
latform (C
s represente
note off”. 
 file in tex
akewalk) [1
d by key n
Here is an 
 
t format 
] 
umber, and
example of
8 
 
 the 
 the 
  
2.5 
 
Her
prov
info
 
Fug
inve
 
Crab
both
exac
 
Figu
 
 
 
 
 
 
 
 
 
 
 
 
Others 
e are some
e that the
rmation hid
e: Using m
nted “coun
 Canon: B
 from begi
tly the sam
re 11 J.S. B
 kinds of m
 music str
den in it. 
athematic
terpoint” to
y Bach ag
nning to en
e music. 
ach's Crab
usic which
ucture cou
al languag
 achieve th
ain, a fanta
d and from
 Canon bas
 have inte
ld be very
e, Fuge m
is. 
stic music 
 end to beg
ed on retro
resting stru
 logical a
usic has
with two d
inning, you
grade transp
cture, these
nd there s
a fractal s
uality parts
 will find t
osition (Ho
 special m
hould be 
tructure. B
. If you pl
hat you pla
fstadter, 1
9 
usic 
rich 
ach 
ay it 
yed 
 
980) 
10 
 
3 THEORETICAL BACKGROUND 
 
3.1 Dimensionality Reduction 
 
3.1.1 Introduction 
 
There are many method of dimensionality reduction, and different ways are for 
different aims. Matrix decomposition is a kind of method which can reduce the 
dimension of data. There are some classic method of reducing the dimension include: 
“Principal component analysis (PCA)”, “Linear discriminant analysis (LDA)”, 
“Independent component analysis (ICA)”, “Vector quantization (VQ)”,  and  
“Singular value decomposition (SVD)”. In the mentioned methods, the original large 
matrixes (which in high dimension space) are all decomposed into low rank matrixes 
(which in low dimension space). But the low rank matrixes have no negativity 
conditions. While we analyze the matrixes, we often cannot explain the negative 
values since it seems meaningless. So there is another special method “Non-negative 
matrix factorization (NMF)”. In this section I will introduce the mathematical theories 
about these methods. 
 
 
3.1.2 Feature 
 
In many applications of machine learning, there are different kinds of data we need to 
deal with, like graphs, texts, audio, video; or experiments’ data of physics, chemistry, 
biology and business. We certainly will not design a special algorithm for each type of 
data, so there are some standard formats of data in machine learning. The standard 
formats are called features. One common format is one point of data corresponding to 
one dimension in Euclidian space. 
 
If the original data cannot be directly used, we should do feature extraction. Actually, 
feature extraction is implemented by dimensionality reduction. We collect the data 
points, and drop the less important ones, leave the most important ones. For example, 
in text processing, when we do sentiment analysis, one method is “Tf-idf” [5]. Tf is 
Term Frequency, and idf is Inverse Document Frequency. It is a weighted score to 
measure how important a piece of text is. 
?? ?? = ???
|?|
|??: ? ? ??| 
Feature selection is actually a special case of dimensionality reduction. Tf-idf is not 
strictly a kind of feature selection, since it does not really drop any low weighted 
dimentions. 
 
The aim for dimensionality reduction is to reduce the high dimensionality and save as 
many as possible the important features. One idea to measure it is reconstruction error, 
11 
 
which is [6]: 
1
? ???? ? ????
?
?
???
 
 
???  is the high dimension reconstruction of the low dimension approximation of ??, 
in other words, the decompression of the compressed term. If the compression method 
is lossless, the reconstruction error will be equal to zero. One aim of dimensionality 
reduction is to let it be as small as possible. 
 
Another method is simply use variance to measure it. For example, if we need reduce 
the D-dimension vectors to 1-dimension, we could maximize the variance, which is 
written as [7]: 
??????
?
1
? ???(??) ? ?(??)????????
?
?
???
 
Here ?  is a dimension reducing function. For 2-dimension case, the second 
dimension should make sure to maximize the variance when it is orthogonal to the 
first dimension. 
 
When we restrict the dimension reducing function ? as linear function, these two 
methods will lead to the same result. 
 
 
3.1.3 Eigenvalue 
 
If a vector ? is an eigenvector of matrix ?, it can be written as: 
?? = ?? 
Here ? is the eigenvalue of the eigenvector ?. A group of eigenvectors is orthogonal. 
Eigenvalue decomposition is to decompose a matrix as the form [8]: 
? = ?∑??? 
Matrix Q includes the eigenvectors of matrix ?. ∑ is a diagonal matrix includes all 
the eigenvalues. 
 
The Matrix multiplication is actually a linear transformation. For example, if we look 
at the matrix M: 
? = ?1 10 1? 
 
 
 It is
This
the 
repr
 
So, 
to th
 
 
3.1.
 
VQ 
stan
cod
 
VQ 
perf
proj
 
 
3.1.
 
LDA
Disc
Bay
prob
train
and 
 
 also a linea
Fig
 is actually
mainly dir
esent the co
the eigenve
e eigenvec
4 Vector 
is widely 
dard JPEG
ing than a k
codes the 
orms fast b
ect. 
5 Linear 
 is a kind 
riminant”, 
es method
ability. Th
 and predi
data minin
r transform
ure 12 The 
 a stretchin
ection of th
mplex tran
ctors mean
tors mean t
Quantizati
used in sig
 and MPEG
ind of dime
sets of poin
ut the accu
discrimina
of supervis
since it is 
s, we may
e word “Di
ct data. It i
g area. 
ation like:
graphical re
g of the coo
e transform
sformation
 the stretch
he importan
on (VQ) 
nal process
-4, they all
nsionality 
ts, use the 
racy is low,
nt analysis
ed learning
invented b
need use 
scriminant”
s a very cla
presentatio
 
rdinate ax
ation, som
s. 
ing “arrow
ce of the a
ing and da
 include th
reduction.
subsets of 
 and this m
 (LDA) 
. In some p
y Ronald F
some prob
 means a m
ssic and p
n of linear 
is in the 2-D
etimes we
s” and the 
rrows. 
ta compres
e VQ step [
them to rep
ethod has t
aper it is a
isher in 19
ability data
odel needs
opular algo
transforma
 plane. Blu
 could just
eigenvalues
sion. Actua
9]. It is mor
resent the 
oo much lim
lso called “
36 [10]. In 
 like prior
 no probabi
rithm in m
tion 
e arrow m
 use the ar
 correspon
lly, in the 
e like a kin
original set
itation for
Fisher’s Li
many kind
 and poste
lity method
achine lear
12 
 
eans 
row 
ding 
ISO 
d of 
s. It 
 our 
near 
s of 
rior 
s to 
ning 
 The
and 
K-c
Wh
 
This
high
Red
dim
line
 
Firs
The
defi
 
The
Her
 
Afte
And
 basic think
the project
lassification
en for all j, 
 function i
 dimension
 points are 
ension line
. 
t let us focu
 LDA algor
ne some ke
 original ce
e ?? mean
r the projec
 the varian
 of LDA is
ed data are 
 problem, 
we have ??
s actually a
 line. When
Fi
in Class A 
, and we c
s on k=2 si
ithm requir
y points. 
ntral point 
s the points
tion, the ce
ce of the po
 projecting 
separated i
there are k l
??(?)
? ?? , we
 kind of pr
 k=2, see F
gure 13 Lin
and Blue p
an see that 
tuation.  A
es the dista
of Class i is
??
 that belong
ntral point 
??
ints in Clas
the tagged 
nto clusters
inear funct
= ??? ? +
 can say tha
ojection. It
igure 13 [4
ear Classif
 
oints are in
it separate
ssume the 
? = ???
nce betwee
 [11]: 
= 1?? ????
  
 to Class i
of Class i i
? = ???
 
s i is [11]: 
data (point)
. It is a kin
ions like [10
??? 
t x belong 
 projects a 
]: 
ier when k=
 Class B, w
d as the or
projection 
 
n different
?
?
 
. 
s [11]: 
? 
 to lower d
d of linear 
]: 
to classifica
high dimen
2 
e project th
iginal class
function is 
 classes is f
imension sp
classifier. F
tion k. 
sion point 
em into a 
ification in
[10]: 
ar. So we n
13 
ace, 
or a 
to a 
high 
 the 
eed 
14 
 
??? = ? (? ? ???)?
????
 
 
Now we can get the loss function of the projection in LDA [11]: 
?(?) = | ??? ? ???|
?
???? + ????
 
 
 
The denominator is an addition of variances, which mean the dispersion of the points 
in the sets. And the numerator is the distance between central points, so maximize J(w) 
we can get the best w. 
 
Now we define a matrix about the dispersion degree of the original data [12]: 
?? = ? (? ? ??)(? ? ??)?
????
 
 
Then the denominator is [12]: 
??? = ? (??? ? ????)? =
????
? ??(? ? ??)(? ? ??)?? =
????
?? ??? 
????+???? = ??( ?? + ??)? = ????? 
 
And the numerator is [12]: 
| ??? ? ???|? = ??(?? ? ??)(?? ? ??)?? = ????? 
 
So the loss function can be formed as: [12] 
?(?) = ?
????
????? 
 
Now we can use the Lagrange multiplier method. We limit the length of the 
denominator as 1 to avoid infinite solution. 
?(?) = ????? ? ?(????? ? 1) 
? ???? = 2??? ? 2???? = 0 
? ??? = ???? 
15 
 
 
Now the problem is an eigenvalue problem and easy to solve. 
 
Now consider K-Classification (k>2) problem, we have the similar form [13]: 
?? = ? ??
?
???
 
?? = ? ??(?? ? ?)(?? ? ?)?
?
???
 
???? = ????? 
 
This is also an eigenvalue problem, if we solve the i-th eigenvector, it is the 
corresponding ??. 
 
Eigenvalue problem is a general problem, its time cost is O(??). We will not discuss 
the detail of the algorithm. 
 
 
3.1.6 Principal component analysis (PCA) 
 
PCA has a close relationship of LDA. The difference is that input data for PCA is not 
tagged. So it is an unsupervised learning method. 
 
We consider two ways to get the same PCA algorithm. First we just consider 
maximize the variance. Assume we still project a point into a vector, the original 
central point is [13]: 
?? = 1? ? ??
?
???
 
 
If we define??  is the vector we project to, the variance is [13]: 
1
? ????? ?? ? ??? ???
? = ??? ???
?
???
 
 
Similarly, use the Lagrange multiplier method [13]: 
??? ??? + ??(1 ? ??? ??) 
 
After the derivation, we have: 
16 
 
??? = ???? 
 
This is an eigenvalue problem, ??  is the eigenvector and ? is the eigenvalue. 
Maximizing it is indeed finding the maximum ??.  
 
Another way is to minimize the loss. First we consider a D-dimension space, assume 
there are D vectors which are orthogonal, we have [13]: 
?? = ? ?????
?
???
 
 
By using approximation method, after the projection we have: 
??? = ? ?????
?
???
+ ? ????
?
?????
 
 
Now we have reduced the dimension to M, and the loss function is: 
? = 1? ???? ? ????
?
?
???
 
 
To minimum this function, we let [13]: 
??
???? = 0 ? ??? = ??? ?? 
??
? ?? = 0 ? ?? = ??
??? 
Then: 
?? ? ??? = ? ?(?? ? ??)?????
?
?????
 
? = 1? ? ? (??? ?? ? ??
???)? = ? ??? ?
?
?????
?
?????
?
???
?? 
 
By using the Laplace multiplier method [14], we get: 
??? = ???? 
 It is
 
As a
dott
pc2 
 
 
PCA
met
And
 
3.1.
 
In P
deco
It re
appr
data
 
In th
 
3.1.
 
We 
this 
the 
of th
 
 
 
 
 the same a
n example
ed lines are
means the 
 is widely
hod is Kern
 maximizin
7 Singula
CA and LD
mpose ma
presents th
oximation 
 compressi
e SVD cha
8 Summa
decide to u
method to 
music struc
e matrix is
s we get in 
, let us con
 the vector
second max
 used in m
el PCA, w
g the covar
r value de
A we intro
trix based o
e original 
can be used
on are relat
pter we wi
ry 
se SVD m
analyze the
ture. Since 
 also consid
the first wa
sider a 2-D 
s we proje
imum eige
Figure 14 A
any areas. 
hich use Ke
iance will n
compositio
duce them
n singular
matrix as t
. In machi
ed to SVD.
ll discuss th
ethod, sinc
 similarity 
we have su
erable. 
 
y. 
space, in F
ct to. pc1 m
nvector, the
 2-D PCA 
But it is a
rnel trick t
ot always 
n (SVD) 
 based on 
 values. It i
he multipli
ne learning
 
e details of
e it is stabl
matrix. The
ch a power
igure 14 it 
eans the m
y are ortho
example [15
 linear tran
o extend P
valid. 
eigenvalues
ncludes ob
cation of s
 area, many
 it. 
e and fast. 
 singular v
ful tool the
is a represe
aximum e
gonal. 
 
] 
sformation
CA to nonl
. But SVD
viously phy
ome smalle
 topics inc
In this proj
alues imply
plan of inc
nt of PCA. 
igenvector
. One exten
inear probl
 is a metho
sical mean
r matrixes 
lude PCA, 
ect we can
 the pattern
reasing the
17 
The 
 and 
ded 
ems. 
d to 
ing. 
and 
LSI, 
 use 
s in 
size 
 3.2 
 
3.2.
 
We 
K-m
deco
to c
 
i
ii
iii
 
The
i
ii
iii
 
And
K-m
Spectral C
1 Introdu
know ther
eans, GM
mposition 
lassical algo
. Spectra
algorith
K-mea
. Since 
unimpo
It is no
have p
most o
baselin
. The al
dimens
using 
optimu
suppor
re are many
. PCA/L
section
. Spectra
. Low-ra
 here is an
eans and s
Figure 15
lustering 
ction 
e have bee
M, etc. Ge
can be call
rithms: 
l algorithm
m), the da
ns 
spectral al
rtant featur
t very sens
roved that 
f the com
e. 
gorithmic 
ional video
EM (Expec
m. Theoret
t theoretica
 branches i
DA as dim
s. 
l Clusterin
nk approxi
 example 
pectral clus
 Selected e
n many me
nerally, all
ed as spectr
s only requ
ta represen
gorithms f
es, it is mo
itive to the
spectral al
parison ex
complexity
/audio/ima
tation-Max
ically SVD
l result and 
n spectral a
ensionality
g/Embeddin
mation 
experiment
tering from
xperiment r
thods of c
 the algori
al algorithm
ires the sim
ts in N-Di
ocus on t
re stable an
 irregular 
gorithms h
periments, 
 is better 
ge data. K
imization)
 has the u
surroundin
lgorithms:
 reduction
g: which w
 result of 
 [19]: 
esult of K-
lustering. C
thms which
. It has m
ilarity ma
mension sp
he major 
d robust th
error in dat
ave better 
K-means 
than K-m
-means is 
iterative m
nique solut
g mathema
: which w
ill be discu
comparing 
means and 
lassical m
 use SVD
any advanta
trix (simila
ace is not
features, a
an the class
a. And ma
performanc
algorithm 
eans, espec
simple, but
ethod ma
ion, and it 
tical proper
e mention
ssed later 
the perform
 
spectral clu
ethods inc
 or eigenv
ges compa
r to K-med
 required a
nd ignore 
ical algorit
ny experim
e. Actually
is used as
ially for 
 it is NP-h
y fall in l
also has m
ties. 
ed in prev
ance betw
stering [19] 
18 
lude 
alue 
ring 
oids 
s in 
the 
hms. 
ents 
, in 
 the 
high 
ard, 
ocal 
any 
ious 
een 
19 
 
TDT2 and Reuters-21578 are widely used text dataset. We can obviously see that 
spectral clustering is much better than K-means. 
 
Mathematically, clustering problem is equivalent to Graph Partition problem [20], 
which means how to partition the vertex set into some disjoint subsets given a graph 
G = (V, E), and meanwhile the partition is “best”. The two difficult points are: 
 
i. It is difficult to define “best”. Some rules are mentioned in [16] 
ii. Even though we have defined a good rule, it is also difficult to optimize it. 
 
There are many ideas about the problem i: 
 
a) Normalized Cut (Graph Cut problem), and some variation like Ratio Cut and 
Min/max cut 
b) Minimum conductance which is highly related to Algebraic Graph Theory [16] 
c) Algorithms without general rules, but can be proved [17] 
d) Not based on graph, but reformulate known clustering methods, to let the 
problem be able to solved by SVD [18] 
 
And here is a simple framework of spectral clustering: 
 
a) Construct a graph, each vertex in the graph corresponding to a data point. 
Connect the similar points. The weight of the edge indicates the similarity 
between the points. Write the adjacency matrix as ? 
b) Sum each row of ? and get ? numbers. Put them on the diagonal line to 
construct a ? ? ? matrix, written as ? and let ? = ? ? ? 
c) Calculate the first ?  eigenvalues of ?  as ???????  and corresponding 
eigenvectors ???????  
d) Combine the ?  eigenvectors together to construct a ? ? ?  matrix, treat 
each row as a vector in k-dimension space, and use K-means to cluster them. 
The result is so related to the original data points. 
 
 
3.2.2 Laplacian Eigenmaps 
 
Laplacian eigenmaps (LE) [21] is the key of spectral clustering. It is actually a kind of 
Dimensionality reduction method, spectral clustering uses it to reduce the dimension, 
and then do K-means. This method only requires the similarity matrix, but also 
requires that the similarity matrix ? should have some local properties, which means, 
for example, if point ? is too far away from point ?, ??? should be equal to zero. 
There are two methods for the local properties: 
 
i. Use a threshold, and set the similarity value to zero if it is lower than the 
threshold. This means we consider locality in ?-domain 
20 
 
ii. For each point, select the nearest k points as its neighbors. And set the 
similarity values to other point to zero. Note that LE requires the similarity 
matrix as symmetric matrix. If ? is one of the k nearest neighbors of ?, 
or/and ? is one of the k nearest neighbors of ?, we keep the ???  value. 
Otherwise set it to zero. 
 
After we build ?, we consider reducing the dimension. We start with the simplest case, 
to reduce the dimension down to 1, as ?? ? ??. So we can minimize the following 
function [21]: 
 
???? ? ???? ???
??
 
 
The meaning of this function is: If ??  is close to ?? , ???  will be large. After 
mapping, if there are large difference between ?? and ??, it will be amplified by ???. 
Minimizing the function makes sure that the mapped points will not be too far away 
from each other when the original points are close. 
 
Let ? be a diagonal matrix which is the sum of each row of ?, and ? = ? ? ? is 
called Laplacian matrix [21]. We should solve the eigenvalue problem: 
 
?? = ??? 
 
Obviously the smallest eigenvalue is zero. The smallest eigenvalue except zero 
corresponding to eigenvector is the result of projection. The eigenvector is 
N-Dimension column vector. After rotation, it is the result of projecting N-Dimension 
original data to one-Dimension. 
 
Extend to M-Dimension case, the only difference is we need to select the smallest M 
eigenvalues except zero. A Matlab code (use KNN to build the similarity matrix) of 
this algorithm is in Appendix A 
 
Actually, Laplacian eigenmap assumes data is distributed on a low dimension 
manifold nested in the high dimension space. And Laplacian matix ? is a discrete 
approximation of manifold Laplace Beltrami operator. We will not discuss the detail 
of it; just consider an interesting example Swiss Roll. 
 
Swiss Roll is just like doughnut. It could be treated as a 2D manifold nested in 3D 
space. In Figure 16 we can see it in the left. In the right there are some randomly 
selected points. 
 Now
We 
plac
 we use bo
can see th
e, but PCA
Fig
th Laplacia
at in the gr
 project the
F
ure 16 Swi
n eigenmap
aph LE ca
 points to a
igure 17 PC
ss Roll and 
 and PCA 
n easily pr
 mixed up g
A vs LE o
random po
to reduce th
oject the d
raph. 
n Swiss Ro
ints 
e dimensio
ifferent poi
ll 
 
n of Swiss 
nts to diffe
21 
Roll. 
rent 
 
 Oth
Emb
smo
near
For 
far f
Her
proj
met
com
thei
erwise, the
edding [22
oth manifo
 points. It f
solving the
rom point 
e ?  is the
ection shou
hod is sim
parison of 
r performan
re is anoth
]. It also u
ld has loca
irstly minim
 best local
?. Then we
 vector a
ld also ke
ilar to LE,
LE and LL
ce on Swis
F
er dimensio
ses the ass
l linear pro
izes: 
? ?
?
 linear refa
 minimize:
? ??
?
fter projec
ep this pr
 which is 
E can be 
s Roll. 
igure 18 LL
nality red
umption o
perty. The 
?? ? ? ??
?
ctored matr
? ? ? ??
?
tion. If ??
operty. Aft
to solve a 
found in [2
E vs LE o
uction meth
f manifold
points can 
????
?
 
ix ?. ???
????
?
 
 can be r
er some tr
eigenvalue
3], and her
n Swiss Ro
od called 
 as LE, an
be refactor
will be ze
efactored, 
ansformatio
 problem. 
e is the ex
ll 
Locally Li
d assumes
ed by the l
ro if point 
the data 
n, the sol
The theore
periment a
22 
near 
 the 
ocal 
? is 
after 
ving 
tical 
bout 
 
23 
 
 
Note that LLE and LE are all nonlinear methods. And the result of PCA is a projection 
matrix. But LLE and LE directly give the result of data, without the “dimensionality 
reduction function”. For new data, they cannot directly use a function to deal with. So 
people found some linear LE and LLE forms, called Locality Preserving Projection 
and Neighborhood Preserving Embedding. 
 
In LPP, the dimensionality reduction function is a linear transformation, represented 
as a dimensionality reduction matrix ?. So the function of LE becomes [23]: 
 
?????? ? ?????? ???
??
 
 
Similar to pervious work, the eigenvalue problem we need to solve is [22]: 
 
????? = ?????? 
 
And get the sorted eigenvectors to build projection matrix ?. In LE, the eigenvalue 
problem has sparseness, so we can efficiently solve it when only several smallest 
eigenvalues are needed. But here after multiplying ?, it is not sparse again. To solve 
this problem we can use a method called Spectral Regression [24]. If we use Kernel 
trick to non-linearize LPP, we will go back to LE.  
 
Otherwise, although LE is an unsupervised method, we can still use the tag 
information. When we build the similarity matrix, the element belong to the same tag 
will have higher similarity value. 
 
In a word, LE is fast, but not always performs best. One special advantage is: when 
outlier occurs, its robustness is especially good. 
 
 
3.2.3 Graph Cut 
 
Graph cut problem is a hot point in spectral clustering. Briefly, it is to cut some edges, 
reconstruct the graph into some independent sub-group. The sum of the weight of the 
cut edges is called Cut-value. For example, imagine segmentation problem will be 
equivalent to cutting the graph into sub-graphs, and we require minimizing the 
Cut-value. In music processing, the music segmentation problem is similar, also can 
be treat as Graph Cut problem. 
 
Actually, Minimum cut is the most famous problem of Graph Cut. But sometimes the 
original minimum cut is not appropriate for practical problem, since the measure 
24 
 
methods are different. For this reason, there are also many different methods, like 
Ratio Cut, Normalized Cut, etc. 
 
Now let us use some formula to describe Graph Cut problem [25]: 
 
First represent the graph as adjacency matrix, written as ?, and ??? is the weight of 
the edge between vertex ? and vertex ?. If no edge, set the weight to zero. Let ? 
and ? be two subsets (without intersection) of graph. So the Cut-value can be 
defined as: 
 
???(?, ?) = ? ???
???,???
 
 
For the simplest case, if we cut the graph into two parts, Minimum cut requires 
minimizing ???(?, ??), which ?? means the complement of ?. But in this case there 
are some isolated points. So we also define RatioCut [20]: 
????????(?, ?) = ???(?, ??)|?| +
???(?, ??)
|??|  
 
And NormalizedCut [20]: 
????(?, ?) = ???(?, ??)???(?) +
???(?, ??)
???(??)  
 
Here |?| means the size (number of element) of ?, and ???(?) = ∑ ?????? . So 
Adding this terms can avoid the isolated points and the cut will be more average. One 
PAMI paper [20] use Normalized cut in image segmentation. 
 
RatioCut and NormalizedCut have very close relationship with spectral clustering. 
But they are still NP-hard. So we can do some deformation [20]. 
 
Let ? be the set of all the vertex in graph. Firstly define a ?-Dimension vector as ?: 
?? =
?
?
?
?
?  ?|??||?|      ??  ?? ? ?
??|?||??|       ??  ?? ? ??
 
 
Remember that in spectral clustering framework, we defined a matrix ? = ? ? ?, its 
name is Graph Laplacian. And it is not the only matrix called this name, many authors 
called their matrix the graph Laplacian. 
  
? h
 
Rela
?
 
 
Def
 ???
imp
 
And
 
The
eige
the 
min
eige
sma
 
Till 
The
?|??
? c
orig
as a proper
ted to the p
??? = ?
?
?,??
ine ?  as t
= ∑ ?? = 0
lies minimi
 we define 
 maximum
nvalue of m
extreme 
imizing ?
nvector is j
llest eigenv
now, it see
 original pr
?|/|?| and 
an be any 
inal solutio
tiy [20]: 
ervious de
???? ?? ? ?
?
= ?
???,??
= 2???
=  2??
= 2|?|
he vector 
 and ????
zing ????,
Rayleigh q
 and minim
atrix . W
value. Sin
(?, ?) . But
ust ?. This
alue and th
ms that we
oblem is N
??|?|/|??
real numb
n, a simple
???? =
finition of 
???
??? ??
|??
|???
(?, ??) ?|??|?
?(?, ??) ?|?
? ????????
which all 
= ∑ ??? =
 with two c
uotient as [2
?(
um value
hen ? eq
ce ??? =
 the smal
 does not s
e correspon
 have solv
P-hard sinc
?|, so that it 
er, which 
 method is 
1
2 ? ????
?
?,???
?, we can h
?|
| + ?
|?|
|??|?
|
| +
|?|
|??| + 2?
| + |??|
|?| +
|?
(?, ??) 
the elemen
? , since 
onditions ?
0]: 
?, ?) = ?
??
?
s of it are 
ual to the 
√?  is c
lest eigenv
atisfy the c
ding eigen
ed the NP-
e the vector
is a discret
means we 
to pay atten
?? ? ???? 
ave [20]: 
?
+ ? ?
????,???
| + |??|
|??| +
ts in it a
|?| is cons
? ? and 
?
??  
the larges
correspond
onstant, m
alue of ?
ondition ?
vector ?.
hard proble
 ? can be 
e problem. 
relaxed th
tion on the
?? ???
|??
|?
2?     
re equal to
tant, minim
??? = √? 
t eigenvalu
ing eigenve
inimizing 
 is zero, 
? ?, so we
m, but this
just assigne
But in our t
e restrictio
 sign symb
?|
| ? ?
|?|
|??|?
?
 1, we h
izing Ratio
e and sma
ctor, it rea
????  imp
so the rel
 use the sec
 is just a tr
d by two v
rick, the ve
ns. To get
ols of the te
25 
ave: 
Cut 
llest 
ches 
lies 
ated 
ond 
ick: 
alue, 
ctor 
 the 
rms 
26 
 
of ?, and corresponding plus and minus symbols to the dicreate terms ?|??|/|?| and 
??|?|/|??|. A more complex method is to use K-means as ? = 2, to cluster ? into 
two groups. 
 
In brief, what we have done is to first calculate the eigenvalues and do K-means to 
find the clusters. Actually, if we generalize the binary case to k-class problem, we can 
get the simple flow of spectral clustering: calculate the eigenvalues and select the first 
k small values, arrange the corresponding eigenvectors to construct matrix and cluster 
them by row using K-means. Here we use a short Matlab code to present spectral 
clustering flow: 
 
 
 
 
 
 
 
 
 
 
 
 
So now we have an approximation solution, it always performs well in practical 
application, but there is no support thesis which has strictly proved this point. 
Moreover, Normalized Cut and Markov chain have very close relationship [26], the 
measure method can be treat as a random walk in graph. 
 
 
3.2.4 Other Topics 
 
3.2.4.1 Minimum conductance 
 
Minimum conductance is another rule of graph partition, which seems similar to 
Normalized cut. But the analysis method is different. 
 
Minimum conductance is a constant in graph, it is named Cheeger constant [27] in 
Algebraic Graph Theory. It traverse all the partition (?, ?), get the value of: 
???(?, ?)
???(???(?), ???(?)) 
 
It seems that it would be more difficult than Normalized Cut, since there is a ??? () 
function idx = spectral_clustering(W, k) 
    D = diag(sum(W)); 
    L = D-W; 
  
    opt = struct('issym', true, 'isreal', true); 
    [V dummy] = eigs(L, D, k, 'SM', opt); 
  
    idx = kmeans(V, k); 
end 
27 
 
function. But interestingly, in Algebraic Graph Theory there is a conclusion, which 
claims that, the second eigenvalue of the Laplace matrix on the graph is an 
approximation of Cheeger constant [25]. 
 
Let Cheeger constant be C [27]: 
2? ≤ ? ≤ ?
?
2  
 
So we can use SVD to solve the last step. This approximation performs worse than 
Minimum conductance, but the difference is not too much, and there are strict 
theoretical boundaries of it in [16]. 
 
 
3.2.4.2 Reformulate the classical clustering methods 
 
Spectral clustering is interesting since not only there are many algorithms about it, but 
also the thinking of it. It combines SVD into optimization problem, which is named 
spectral relaxation. For example, [18] used this thinking to reformulate K-means 
algorithm. He discretizes the continuity and relaxes the problem, such that it can be 
solved by SVD finally. 
 
 
3.2.4.3 Others 
 
For some methods, it is hard to describe their rules, and some authors only gave the 
algorithms and proofs. 
 
[16] gives a recursive algorithm, keep cutting the graph into two parts, until the 
terminal conditions are satisfied. It can deal with K-class problem. And [16] also 
gives the proof of boundaries which cannot be calculated for Normalized Cut. 
 
[17] gives a method to solve K-class problem directly. It does not include the rules of 
the aim function, but it uses the Matrix perturbation theory to prove the algorithm 
works in some cases. 
 
In [28], spectral algorithm is used to cut the point sets as trees, and use other rules 
(like K-means) to merge the leaf. So that it cluster the data. And for trees, dynamic 
programming can be used to solve NP-hard problems. 
 
Semi-definite Programming is another way, if we relax some problems on SDP, the 
result will be better. But in [18], we cannot find the difference by the experiments. 
And SDP is much more complex. 
 
28 
 
4 RELEVANT WORKS 
 
4.1 The Previous Project 
 
This project is based on the previous project “Finding harmonic structure in music 
files” by Tom Lyner [4]. Some of the ideas are from his project, include the 
12-dimensional vector, note and bar modeling, similarity matrix, using MIDI files, etc. 
In our project, we are more focus on the decomposition of matrix part, research of the 
SVD method. We hope by using SVD, we can add more functions and do more 
experiments about music structure, not just focus on the harmonic similarity, because 
SVD can break many limitations in the previous project [4]. Tom had implemented 
software by JAVA, so we also decide to use JAVA for the implementation, and then 
we can reuse the basic part of Tom’s software. Since our project is more about the 
SVD method, we may not cost too much time on implementation of the software, and 
we may also use Matlab, R language or other tools to do some independent 
experiment and some quick tests. 
 
4.2 Music Generation 
 
In the early time, music generation was the primary topic in music information 
retrieval area. Programs are created to generation different kinds of music. And some 
composers used computer as a music band, they created and computer played. But 
people found that music generation was actually the most difficult problem in music 
information retrieval area. Most programs can just make the similar music or mix 
some different piece of music to try to create a “new” music.  
 
Generating accompaniment then was mentioned. It is easier since it just need 
computer listen to the music and generate some music that fit it. Since computer is 
fast, so we can let it real time generate the accompaniment. The harmonic structure 
analysis will occur in this work, computers see the music as structure include patterns. 
 
4.3 Music structure analysis 
 
People can listen music and “feel” it, what about computers? Some people had tried to 
make computer to be able to “feel” the music, not just treat music as data. 
Dannenberg (2002) tried to let computer classify the different style of the music, and 
the techniques he used is from machine learning area. Y Shiu and H Jeong(2006) used 
the similarity matrix; they analyze the music by Viterbi Algorithm, and they can 
recognize some chords in the music. Many people have work on similarity matrix, 
like Dannenberg and Hu’s (2002). They use the matrix to find the clusters. Foote and 
Cooper’s (2001) built a visualization of similarity matrixes; they first used FFT to 
filter the signal and then tried to find the self-similarity part in the music. Our project 
can treat this relevant work as reference, and focus on how to construct a general 
method of structure discovery in music. 
29 
 
5 METHOD 
 
In this section we describe and discuss the details of our experiments and software 
implementation. 
 
5.1 Introduction 
 
In previous project [4], we decide to use bar as the basic unit, since a bar can be 
represented as a chord, that means a bar is seen as a harmonic “idea”. By using the 
method by Paiement et. al. (2005) we consider representing a note as a 
12-dimensional (one for every semitone) space vector. So we can also represent the 
whole bar as a vector addition of all the notes in the bar. While every bar is just a 
vector, we can calculate the distance of each pair of the bars. Then we can build a 
similarity matrix which measures the difference of these bars. And we can find some 
patterns in this matrix. 
 
We decide to keep this method at first, but this method has some problem, we will try 
to improve it. It mixes all the notes in one bar as one vector. That is good for reducing 
the size of the matrix. But the notes in a bar lost their order information. It is just like 
in each bar we press all the notes in the same time. This may induce losing much 
information. So we can try to use note or segment or other things as the basic unit to 
construct the matrix. Another problem is the method to compute distance. In previous 
project we use Euclidean distance [4]. In this project we can try some more kinds of 
distance. 
 
And we will use SVD as a matrix decomposition method to analyze the musical 
matrix. This is the main innovation point of this project, since in previous project [4] 
the segmentation and clustering process are done by simple algorithms and the result 
of SVD is very interesting to musical structure discovery. We can use it to cluster the 
segments based on the segment-segment self-similarity matrix. We research some 
applications of SVD in musical matrix decomposition, and analyze the mathematical 
meaning and musical meaning of them. And we study and analyze the details of 
algorithms of SVD, to know the theoretical derivation of SVD and why it can be used 
in musical structure discovery. We also design some experiments and survey the 
performance of SVD. 
 
There are some interesting topics we discuss. The first is matrix selection. We know 
that many applications about music analysis are based on matrix, so different matrixes 
are used for different researches. For example, Segment-Segment self-similarity 
matrix can be used for clustering, but Song-Note similarity matrix can be used for 
musical retrieval. Other topics include distance function selection and vector space 
selection, by using different spaces and distance function; we can solve different 
problems about music. 
 
 5.2 
 
As t
actu
we c
sem
of th
 
 
The
repr
note
 
Not
posi
So t
 
Modeling 
hat mentio
ally many 
an fold har
itones with
e 12 dimen
Fig
n we can 
esented as 
 C#, etc. In
e that the v
tions to rig
here are tot
ned in the p
frequencies
monics. W
 different s
sions are s
ure 19 A di
define a 1
a vector. T
 the illustra
ector of no
ht. The high
ally 12 kind
revious pr
 in it inste
e decide to 
trength in t
hown in Fig
stribution o
2-dimensio
he first dim
tion we can
te D is ver
est value a
s of vector
oject [4], wh
ad of one (
use the met
he octave. 
ure 19: 
f the streng
nal space
ension of t
 see note C
y similar to
ppears in C
s. 
en we liste
which we 
hod that re
An distribu
th of the 12
[4]. In this
he space is
 and not D
 which of 
 for note C
n to one o
generally b
presenting e
tion graph 
 dimension
 space eac
 note C, an
as example
note C. It j
 and in C# 
ctave, there
elieve in). 
ach note b
of the stre
s [4] 
h note can
d the secon
. 
ust shifted 
for note C#
30 
 are 
And 
y 12 
ngth 
 
 be 
d is 
two 
, etc. 
  
 
Figure 20 They rep
Figure 2
resent the v
1 The selec
ectors of N
 
ted notes bu
ote C in a) 
ild a bar’s 
 
and Note D
vector [4] 
 in b) [4] 
 
31 
  
If w
con
note
The
 
And
distr
exp
 
 
5.3 
 
By u
song
 
So w
mus
mat
y b
com
 
e want to u
sider only 
s in a bar t
 long notes
 here we 
ibutions, j
eriments to 
Building th
sing the E
. Here is th
D(x, y
e can try t
ical sequen
rix is n ? n
y calculatin
pared to ev
Figure 2
se bar as t
the notes in
ogether to 
 and the sho
have anot
ust leave th
compare th
e matrix 
uclidean di
e function:
) = ?(????
o build a se
ce with itse
. The valu
g the dista
ery unit inc
2 A diagram
he basic un
formation 
calculate th
rt notes wi
her choice
e stronges
e differenc
stance func
 [4] 
?)? + (???
lf-similarity
lf. If we de
e of (?, ?)
nce or sim
lude itself
 of compar
it, we can 
without tim
e vector of
ll be treated
, we can 
t semitone
e. 
tion we can
??)? + ?
 matrix. T
fine n is t
 in the mat
ilarity. Eac
[4]. 
ison betwe
also model
ing inform
 a bar, as w
 equally. 
consider no
 as 1, set 
 calculate e
+ (?????)?
he self-simi
he length o
rix is the c
h basic un
en two basi
 the bars. H
ation. So 
hich show
t represen
others to 0
ach pair of
= ??(?
?
???
larity matri
f self-simila
omparison b
it in the se
c units x a
ere we sim
we add all
n in Figure
t the note
, and do s
 the bars in
????)? 
x compares
rity matrix
etween x 
quence wil
 
nd y [4] 
32 
ply 
 the 
 21. 
s as 
ome 
 one 
 the 
, the 
and 
l be 
 The
mem
see 
mat
 
We 
dist
dist
them
 
We 
exam
piec
 
5.4 
 
As a
 
5.4.
 
To c
abou
are 
bou
patt
chec
 matrix the
ory to kee
the pattern
hematical t
have ment
ance funct
ance, Cheb
 in the dis
will also t
ple we ca
e of music 
Clustering
 simple ex
1 Segmen
luster the 
t audio/mu
checkerbo
ndary betw
erns in th
kerboard k
n is symme
p the matr
s by eyes,
ools. 
ioned that w
ions used 
yshev dista
cussion sec
ry to build
n compare 
in the datab
 
ample, we w
tation 
music, seg
sic segmen
ard pattern
een the seg
e matrix, 
ernel [30], a
Figure
trical along
ix. The rea
 and this k
e will try 
in differen
nce, Minko
tion. 
 some oth
too differen
ase. 
ill researc
mentation w
tation. He
s [29] whe
ments will
we use a 
n example i
 23 An exam
 the diago
sons of usi
ind of ma
some othe
t subjects 
wski distan
er matrix, 
t songs inst
h the simila
ill be don
re we just 
n the segm
 show as a 
matched 
s shown in
ple check
nal line. So
ng similari
trix can be
r distance f
like Eucli
ce, etc. We
not just se
ead the sam
rity based c
e firstly. T
use a simpl
ents chan
checkerboa
filter by u
 Figure 23.
erboard ker
 we only n
ty matrix is
 analyzed 
unctions. T
dean dista
 will discu
lf-similarity
e songs, or
lustering o
here are m
e method. 
ge. In oth
rd pattern.
sing a G
nel [29] 
eed half of
 we can ea
be using m
here are m
nce, Hamm
ss and com
 matrixes. 
 search a s
f segments
any algorit
Note that t
er words, 
 To find al
aussian-tap
 
33 
 the 
sily 
any 
any 
ing 
pare 
For 
mall 
. 
hms 
here 
the 
l the 
ered 
  
By 
valu
the 
 
 
Sinc
keep
 
 
Actu
and 
 
 
5.4.
 
In s
use 
 
 
using the k
e” which m
calculation 
Figure 24
e this meth
 all the ori
??(?
ally we do
appropriate
2 Applica
ection 5.3, 
cosine dista
ernel along
eans how n
by the kern
 Novelty sc
od only re
ginal matrix
, ?) = ? ?
 this in a 
, the reduc
tion of SV
we talk abo
nce as the 
 the diagon
ovelty the 
el for time-
ore comput
quires the 
, but const
?, ? + ? ?
“lag domai
tion is very
D 
ut matrix b
distance me
??????
al of the m
local piece
indexed se
ed by the k
diagonal st
ruct a resiz
??2??      ?
n” to save 
 appreciabl
uilding. W
asure: 
?, ??? =
?
|
atrix, we 
s are. And F
quence. 
ernel on the
rip of the m
ed N ? K m
= 1, . . , ?,
time and m
e. 
e use bars 
??, ???
??| ????
 
can calcula
igure 24 sh
 similarity 
atrix, we 
atrix ?? [3
   ? = 1, . .
emory. Wh
as the basic
te the “nov
ows a resu
 
matrix [29] 
do not nee
1]: 
, ? 
en ? is s
 units. And
34 
elty 
lt of 
d to 
mall 
 we 
35 
 
Of course some other methods are also valid, such as non-negative exponential 
distance measure: 
???????, ??? = exp ?1 ? ???????, ???? 
 
For our method, cosine measure is good enough. After we calculate the self-similarity 
matrix, we can detect the boundaries of segments. But the lengths of segments are 
different, so we have to design a method to measure the similarity between the 
variable segments. 
 
We need to design a segment-indexed similarity matrix [31], which measures the 
similarity of segments, written as ?? . This similarity is computed by the 
Kullback-Leibler (KL) distance of the Gaussian density. The KL distance of 
B-dimensional normal densities ?(?? , ??) and ???? , ??? is [31]: 
??? ??(?? , ??) ? ???? , ????
= 12 ??? ?
????
|??|? +
1
2 ???????
??? + 12 ??? ? ?? ?
?∑?? ???? ? ?? ? ?
?
2 
 
?? means the matrix trace. Because d??() is nonsymmetrical, so we modify it to 
another form which is symmetrical. We just add two KL distance terms together and 
the sum is [31]: 
???? ??(?? , ??) ? ???? , ????
= ??? ??(?? , ??) ? ???? , ???? + ??? ????? , ??? ? ?(?? , ??)?
= 12 ???????
??? + 12 ???????
???
+ 12 ??? ? ?? ?
??∑?? ? + ∑?? ????? ? ?? ? ? ? 
 
So that we can represent segments by mean ? and covariance ∑. And define the 
similarity of segments ?? and ?? as [31]: 
 
???????, ??? = ??? ?? ???? ??(?? , ??) ? ???? , ????? 
 
It can be equal to the value between zero and one. And it is symmetrical now, which is 
required by the similarity matrix construction. Now the matrix can be built as [31] 
??(?, ?) = ???????, ??? 
?, ? = 1, . . , ? 
36 
 
 
By using SVD, we can write the similarity matrix ?? as the form [32]: 
?? = ???? 
Here we will not discuss the meaning of SVD and other details. We put these in the 
discussion section. Note that ? and ? are all orthogonal. Matrix ? only includes 
the singular values sorted along the diagonal line. So singular value ?? = ???. We can 
now write the form as the sum of sub matrixes [32]: 
??(?, ?) = ? ??
?
???
?(?, ?)?(?, ?) 
 
And we can define a series of matrix ?? as [31]: 
?? = ???(?, ?)?(?, ?) 
So ??(?, ?) = ∑ ??????  
To cluster the segments, we calculate the measure function ?? [31]: 
??(?) = ? ??(?, ?)
?
???
        ?, ? = 1,  , ? 
The character ? means the ?-th group. So there are totally ? groups. The value of 
??(?) measures the similarity of the segment ? to all the segments in the ?-th group. 
This thinking is very similar to K-means, which also measure the distances from each 
point to each cluster. So we can indicate the segment ? to group (cluster) ? which 
has the highest value of ??(?). 
 
Till now we can give a simple algorithm description [32]: 
 
Algorithm 1 Clustering by SVD  
 
1. Calculate and construct a Bar-Bar self-similarity matrix ? 
2. Build a segment-indexed similarity matrix ?? 
3. Use SVD algorithm to decompose the matrix ?? and calculate all ??(?) 
4. Indicate the segment ? to group ? when: 
? = ??????
???,..,?
??(?) 
 
 
 5.4.
 
Let 
No. 
self
F
 
Sinc
here
A?, 
a ki
 
Not
com
com
sim
con
 
By 
patt
two
the 
3 Examp
us use a pi
1 in G maj
-similarity m
igure 25 Th
e the first 
 we on
A?, A?, A?,
nd of low d
e that the 
ponent ma
ponent is a
ilar to K-m
sidered. 
watching th
ern, which 
 or three gr
singular val
le Experim
ece of mus
or - BWV 1
atrix: 
e self-simi
several sub
ly select 
 A?, A?, A?
imensional
first sub-m
y not repre
lso valid, s
eans, eac
e seven gr
indicate to
oups, and f
ues are dec
ent 
ic as an ex
007 since 
larity matri
-matrixes 
the first
 and sorted
 approxima
atrix somet
sent the lat
ince it is ba
h compone
aphs in Fig
 the simila
or A?and 
reasing wh
ample of th
it is short a
x of Bach-
can repres
 7 sub-
 by the sin
tion of the 
imes can b
ent meanin
sed on the 
nt with s
ure 26, we
r segments
 A? the pa
en the num
is algorithm
nd well str
Suite No. 1 
ent the maj
matrixes 
gular value
original ma
e ignored 
g of data. B
quadratic p
ignificant 
 can see th
. The patte
tterns becom
bers of sub-
. We choo
uctured. Fir
in G major
or of the o
which are
s. This can 
trix. 
since the m
ut in our p
rogrammin
singular va
at each sub
rns are sig
e vague. T
matrixes ar
se Bach- S
st we build
 - BWV 100
riginal ma
 marked 
also be see
ost impor
roject, the 
g problem,
lue should
-matrix ha
nificant in 
hat is bec
e increasin
37 
uite 
 the 
 
7 
trix, 
as 
n as 
tant 
first 
 and 
 be 
s its 
first 
ause 
g. 
  
 
Figure 26 The first 7 sub-matrixes computed by SVD 
38 
 
 
 
 And
Alg
wat
 
And
valu
 
 
5.4.
 
The
basi
mai
the 
 
Figu
The
man
anal
 
 
 
 by constru
orithm 1, w
ch ?? valu
 we can no
e in the gra
4 Softwa
 software i
c framewor
nly focus o
software. 
re 28 show
 detailed d
y impleme
yzed in the
cting the m
e can calcu
es related t
Figur
w finish t
ph. 
re Implem
s based on 
k of MIDI
n the softw
s the Class
escription c
ntation deta
 discussion
atrix ?? a
late the m
o A?, A?, A
e 27 The ?
he clusterin
etation 
the previou
 input and 
are implem
 diagrams o
an be foun
ils in this r
 section. 
nd matrix 
easures of t
?, A? in Fi
? values o
g. We just
s project [
graph draw
entation, 
f the basic 
d in his re
eport. But 
??(?) usin
he “cluster
gure 27. 
f A?, A?, A
 indicate ea
4] by Tom 
ing functio
we implem
framework
port [4], an
the implem
g the meth
ing scores”
?, A? 
ch segmen
Lyner. He 
n. Since thi
ent an upd
 form Tom 
d we will 
entation al
od describe
. Here we 
t to the low
implements
s project is
ated versio
Lyner’s pro
not discuss
gorithm wi
39 
d in 
only 
 
est 
 the 
 not 
n of 
ject. 
 too 
ll be 
 And we redesig
Figu
n the GUI 
F
re 28 Class 
to show SV
igure 29 T
diagram fo
 
D sub-mat
he GUI of t
r the softwa
rixes, as sho
he softwar
re [4] 
wn in Figu
e 
re 29:  
40 
 
 
 5.4.
 
The
sect
 
5.4.
 
We 
(LS
be e
disc
 
In L
Bar
 
 
So w
proj
This
5 More A
re are man
ion we will
5.1 Musica
know that 
I). It uses S
asily done
overy. 
SI, we co
-Feature ma
Fe
Bar 
Bar 1 
Bar 2 
Bar 3 
 
e can use
ect the Bar
 can be use
Figure
pplication
y other ap
 study som
l Retrieval 
in text ret
VD to deco
. We consi
nstruct the
trix, like th
ature C



 
Figur
 SVD to c
s to a lower
d in the mu
 31 The low
s 
plications 
e of them.
rieval, ther
mpose the 
der use th
 document
e form sho
 
 
 
 
e 30 The fo
ompose the
 dimension
sic databas
er space af
of SVD in
e is a meth
document-t
e variation 
-term matr
wn in Figur
C# 
 
 
 
 
rm of a Ba
 matrix an
al space as 
e, or music
ter the dim
 musical st
od called 
erm matrix
of this me
ix. For our
e 30: 
D    
 
 
 
 
r-Feature m
d calculate
shown in F
 piece searc
ensionality 
ructure dis
Latent Sem
 and the tex
thod in mu
 project, w
      
 
 
 
 
atrix 
 the latent 
igure 31 to 
h. 
 
reduction b
covery. In 
antic Inde
t searching
sical struc
e could u
properties, 
do the retri
y SVD 
41 
this 
xing 
 can 
ture 
se a 
and 
eval. 
42 
 
5.4.5.2 Automatic Summarization 
 
After we get the clusters of the segments by using algorithm 1, we can design a 
method to summarize the pieces. In first step we select the two clusters with first two 
biggest singular values. For major cluster i, we can define index ??? by calculating the 
terms of ?? [31]: 
??? = ?????????,,? ??(?) 
In other word, we can summarize each cluster by using this index. And if we can use 
only one segment represent each cluster, we summarize the whole song. This can also 
be used to predict the type of clusters, like verse, chorus, etc. 
 
 
 
5.4.5.3 Audio Segmentation 
 
In our project the segmentation process are done by a simple method. Actually, SVD 
can also be used to segment the music. In the paper [33] they mentioned a method. 
This method use Markov Normalization to convert the matrix to probability matrix, 
and use SVD to compose the self-similarity matrix. 
 
5.4.5.4 Others 
 
In above we have introduced many applications about SVD in music structure 
analysis. Note that the main different of them is the matrix. Using different matrix we 
could solve different problems. For example, Bar-Bar and Segment-Segment 
self-similarity matrix are used for clustering. Bar-Feature and Segment-Feature matrix 
are used for musical retrieval. Song-Note can be used to musical feature extraction. In 
our project we will do experiments most about clustering, which is an extension of the 
previous project [4]. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
43 
 
6 RESULT 
 
6.1 Clustering Experiments 
We use some pieces of music to do the clustering experiments. The selected pieces 
are: 
 
1. Menuet 1& Menuet 2, ‘Cello Suite no.1, J. S. Bach 
2. Blues in E (Piano, bass, guitar, drums), Anon. 
3. Canon in D, J. Pachelbel 
4. Allegro, Eine Kleine Nachtmusik, W. A. Mozart 
5. Andante, Piano Sonata in G, K.283, W. A. Mozart 
6. Allegro, Symphony no. 6 ‘Pastorale’, L. v. Beethoven. 
 
They are the same as those in the previous project [4], such that we can compare the 
performance between our project and the previous project. 
 
Piece 1 
Menuet 1& Menuet 2, ‘Cello Suite no.1, J. S. Bach 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Repeating sections are: 
---------------------- 
Bars 9 - 16 match 1 - 8: length of 8 
Bars 97 - 104 match 1 - 8: length of 8 
Bars 97 - 120 match 9 - 32: length of 24 
Bars 33 - 48 match 17 - 32: length of 16 
Bars 105 - 120 match 33 - 48: length of 16 
Bars 53 - 54 match 49 - 50: length of 2 
Bars 57 - 64 match 49 - 56: length of 8 
Bars 61 - 62 match 49 - 50: length of 2 
Bars 57 - 58 match 53 - 54: length of 2 
Bars 61 - 62 match 57 - 58: length of 2 
Bars 81 - 96 match 65 - 80: length of 16 
The form of this piece is: 
------------------------- 
A : bars 1 to 8 (8 bars long) 
A : bars 9 to 16 (8 bars long) 
B : bars 17 to 32 (16 bars long) 
B : bars 33 to 48 (16 bars long) 
C : bars 49 to 56 (8 bars long) 
C : bars 57 to 64 (8 bars long) 
D : bars 65 to 80 (16 bars long) 
D : bars 81 to 96 (16 bars long) 
A : bars 97 to 104 (8 bars long) 
B : bars 105 to 120 (16 bars long) 
Clusters: 
---------------------- 
A:  
bars 1 to 8 (8 bars long) 
bars 9 to 16 (8 bars long) 
bars 97 to 104 (8 bars long) 
B: 
bars 17 to 32 (16 bars long) 
bars 33 to 48 (16 bars long) 
bars 105 to 120 (16 bars long) 
C: 
 bars 49 to 56 (8 bars long) 
 bars 57 to 64 (8 bars long) 
D: 
 bars 65 to 80 (16 bars long) 
 bars 81 to 96 (16 bars long) 
 
The form of this piece is: 
------------------------- 
AABBCCDDAB 
 
Style Prediction: 
------------------------- 
Sonata 
44 
 
The report of previous project [4] is on the left and that of our project is on the right. 
We can see our method and the previous method performs the same, but our method is 
even faster. Additionally, our method gives the style prediction which based on the 
musical form. This is a well-structured music, so most algorithms can perform well 
and get the right musical form. 
 
 
Piece 6 
Allegro, Symphony no. 6 ‘Pastorale’, L. v. Beethoven 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
Clusters: 
---------------------- 
A:  
bars 1 to 16 (16 bars long) 
bars 17 to 32 (16 bars long) 
bars 57 to 75 (19 bars long) 
bars 118 to 143 (26 bars long) 
bars 223 to 254 (32 bars long) 
 
B: 
bars 33 to 56 (24 bars long) 
bars 33 to 48 (16 bars long) 
C: 
 bars 76 to 96 (21 bars long) 
bars 97 to 117 (21 bars long) 
D: 
 bars 144 to 191 (48 bars long) 
 bars 192 to 222 (31 bars long) 
 
The form of this piece is: 
------------------------- 
AABACCADDA 
 
Style Prediction: 
------------------------- 
Rondo 
 
The form of this piece is: 
------------------------- 
A : bar 1 (1 bar) 
B : bars 2 to 16 (15 bars long) 
C : bar 17 (1 bar) 
B : bars 18 to 32 (15 bars long) 
D : bars 33 to 53 (21 bars long) 
E : bars 54 to 56 (3 bars long) 
? : bars 57 to 59 (3 bars long) 
F : bars 60 to 72 (13 bars long) 
? : bars 73 to 75 (3 bars long) 
G : bars 76 to 80 (5 bars long) 
? : bars 81 to 83 (3 bars long) 
H : bar 84 (1 bar) 
I : bars 85 to 88 (4 bars long) 
? : bars 89 to 91 (3 bars long) 
J : bars 92 to 98 (7 bars long) 
? : bars 99 to 102 (4 bars long) 
K : bars 103 to 106 (4 bars long) 
? : bars 107 to 109 (3 bars long) 
L : bars 110 to 114 (5 bars long) 
? : bars 115 to 117 (3 bars long) 
M : bars 118 to 138 (21 bars long) 
? : bars 139 to 143 (5 bars long) 
N : bars 144 to 191 (48 bars long) 
O : bar 192 (1 bar) 
? : bars 193 to 202 (10 bars long) 
P : bars 203 to 207 (5 bars long) 
Q : bars 208 to 221 (14 bars long) 
R : bar 222 (1 bar) 
? : bars 223 to 252 (30 bars long) 
S : bars 253 to 254 (2 bars long) 
45 
 
Now let us skip over piece 2 to piece 5, and watch piece 6 directly. This is indeed a 
hard piece for the previous project. Tom Lyner claimed that there are problems when 
he tried to identify the musical form. And he finally only found one pattern “B”. And 
look at our result, our method cluster the segments into four groups and successfully 
predict the musical style. 
 
Why our method performs much better than the previous method? The only reason is 
SVD. Since this music is written by Beethoven, it includes less obvious structure than 
the music written by Bach like piece 1. And this song is longer and more complex, 
include many musical composition tricks. So SVD can reduce the dimension of the 
data and find the latent relationship and clusters. 
 
We compare our method and the method in the previous project [4], and we use the 
manual musical form and segmentation as reference. The statistical results for the six 
pieces are in the form below: 
 
Songs Old Method Our Method 
Segmentatio
n precision 
Musical form 
precision 
Segmentatio
n precision 
Musical form 
precision 
Style 
Prediction 
Piece 1 94.1% 100% 97.2% 100% right 
Piece 2 81.7% 66.6% 77.2% 80% wrong 
Piece 3 61.4% 46.5% 83.6% 100% right 
Piece 4 92.3% 100% 89.7% 100% right 
Piece 5 45.6% 66.6% 70.2% 90% wrong 
Piece 6 21.9% 16.5% 93.9% 100% right 
Table 2 Comparison between our method and the old method 
 
Here the precision means the average precision. For example, if the right (manual) 
musical form is ABCAB, and the result of one method is ABCAA, the precision will 
be 80%. 
 
From the form we can see that our method is much better than the previous one. And 
the improvement is over 100% of the original method. 
 
We also compare our method to other methods, but most experiments are designed for 
pop music, and our method is designed for classical music. So in the experiments of 
pop music our method performs worse and in the experiments of classical music our 
method performs better than most methods. 
 
46 
 
6.2 Distance function Selection 
 
In earlier sections we mentioned the distance functions. Here we do some experiments 
to test their performance. By replacing the cosine distance function with some other 
functions, we modify the similarity matrix constructed in Chapter 5. And the 
evaluation method is just as we used in the clustering experiments, to measure the 
musical form precision. 
 
We test five functions: 
Cosine distance 
Euclidean distance 
Hamming distance 
Chebyshev distance 
Minkowski distance 
 
We use cosine distance as the base line, and the details about how there functions 
defined can be found in [34]. And we use the open source implementation of these 
distance functions to do the experiments. The sample musical pieces are also the six 
pieces. 
 
Distance Function Piece 1 Piece 2 Piece 3 Piece 4 Piece 5 Piece 6 
Cosine distance 100% 80% 100% 100% 90% 100% 
Euclidean distance 90% 90% 80% 80% 100% 90% 
Hamming distance 60% 50% 60% 80% 100% 80% 
Chebyshev distance 90% 100% 100% 90% 60% 100% 
Minkowski distance 100% 100% 100% 80% 100% 70% 
Table 3 Comparison of different distance measures 
 
From form 2 we can see that cosine distance is averagely good enough for most cases, 
and it is the most stable method. Hamming distance is not appropriate for this method. 
Chebyshev distance and Minkowski distance is good for most samples, but they are 
not stable enough. Euclidean distance is stable, but only one result is 100%, seems not 
good enough. So using cosine distance is really a good choice in our project. 
 
 
 
 
 
 
 
 7 
 
7.1 
 
In C
mos
Eige
sym
mat
be s
The
man
repr
 
 
Not
appr
 
 
The
low
repr
DISCUSSI
Meaning o
hapter 3 w
t obvious d
nvalue pro
metry. But
rixes. SVD
hortly writt
se equation
y times in
esent SVD 
e that the d
oximation 
 interesting
er space. F
esent the se
ON 
f SVD 
e introduc
ifference b
blem can 
 for most
 is designed
en as [35]: 
s for ? ?
 the future
by illustrat
ark area me
by the spar
Fig
 approxima
or examp
gments in t
e eigenvalu
etween eige
only treat 
 general p
 for these 
? matrix ?
 since it 
ions: 
Figure 32
ans the spa
seness as:
ure 33 Illu
tion means
le, in the 
he lower (s
es. But SV
nvalue and
the square 
roblems, t
cases; it ca
 
 you may
is very im
 Illustratio
rseness of t
stration of 
 we can rep
musical s
elect the fir
D is base
 singular v
matrix, an
hey cannot
n deal with
 have seen 
portant for
n of SVD
he matrix. 
Reduced SV
resent the 
egments cl
st two dim
d on singu
alue is the a
d sometim
 be conve
 matrix in 
 
before. And
 SVD. An
So we can d
D 
high dimen
ustering se
ensions) sp
lar values. 
ccepted m
es requires
rted to sq
any size. It
 you will s
d we can 
 
o the low-
 
sional data
ction, we 
ace as: 
47 
The 
atrix. 
 the 
uare 
 can 
ee it 
also 
rank 
 in a 
can 
  
Now
to lo
spac
 
 
7.2 
 
7.2.
 
Def
TA A
The
 
 we have t
wer dimen
e. 
SVD Analy
1 Introdu
inition 1. A
 is the sin
orem 1. Let
F
he geometr
sional spac
sis 
ction 
ssume A?
gular value
m nA R ?? , t
[ 1U u=
S
Here
igure 34 Se
ic meaning
e. And som
m nR ? , the 
 of A , all th
here must e
],..., mmu R?
uch that: U
 r diag? =
gments in t
 of SVD: it
e data can 
Nonnegativ
e singular 
xist orthog
m?  And V
0
T
r
AV
??
= ??
1( ,..., ),r? ?
he low spac
can project
be easily se
e square 
values of A
onal matrix
[ 1,..., nv v=
0
0
r
n r
r
m r
?
??
??
1 ... r? ?≥ ≥
e 
 the high di
parated in t
root of the
 is written
es [36]: 
] n nR ??  
?1 
0>  
mensional 
he dimensi
 eigenvalu
 as ( )A? . 
48 
 
data 
onal 
e of 
[36] 
49 
 
The equation ?1 is called the Singular value decomposition (SVD) of matrix A , i?  
is the singular value of A . iu  and iv  are the i-th left singular vector and the i-th 
right singular vector. From this, we can get some useful reasoning. 
 
Reasoning 1. Let m nrA C
?? , the number of the non-zero singular values of A  is
( )r rank A= , 
1,...,r nv v+  is a standard orthogonal basis of the null space(or kernel) of A  
1 ,..., ru v  is a standard orthogonal basis of the domain space of A  
1
r
H
i i i
i
A u v?
=
=?  is the full rank SVD of A . 
 
And we define the following notations: 
 
?? is the i-th singular value of A  
???? is the maximum singular value of A  
???? is the minimum singular value of A  
 
Now let us consider the geometric meaning of the singular value. Assume m=n, we 
have  [36]: 
{ }2: , , 1n nnE y C y Ax x C x= ? = ? =  
It is a hyper-ellipsoid. There are n semi axis which can be seen as the n singular 
values 1 2 ... 0n? ? ?≥ ≥ ≥ ≥ of A . 
 
Let us assume m n≥ (if m n< , we can use the transposition of A , and do the SVD as 
well, then calculate the transposition of the result of SVD). If we represent U  as: 
1 2( , ),U U U=  
Then we can get a new SVD equation: 
1
TA U V= ?  
Here 1U  has n  m-Dimension orthogonal vectors, 1 2( , ,..., )ndiag ? ? ?? = , 
1 2 ... 0n? ? ?≥ ≥ ≥ ≥  is the singular values of A  
 [36] 
 
50 
 
7.2.2 The application of SVD 
 
1) Determine the rank of matrix 
 
When the rank of A  is r , we have: 
1 1... ... 0r r n? ? ? ?+≥ ≥ > = = =  
[37] 
Then if we find 0r? ≠  and 1 ... 0r n? ?+ = = = , we can determine the rank of 
matrix. 
 
Actually, the singular values are almost all non-zero. We should also 
determine the approximation rules. 
 
2) Determine the projection operator [38] 
 
If the rank of A  is r , we can represent 1U  as 
( )(1) (1)1 1 2,U U U=  
Here (1)1U  is a m r?  matrix, and the column vectors of it form the 
orthogonal vector base of the column space of A  
 
So (1) (1)1 1
T
AP U U=  is the orthogonal projection operator on the column space 
of A and (1) (1)2 2 2 2( , )( , )
T
AP U U U U
?
=  is projection on the orthogonal 
complement space of the column space of A  
 
3) Least squares (LS) problem [39] 
 
The LS problem is: Let m nA R ?? ( ), mm n b R> ? , find nx R?  that: 
2 2
min{ : }.nAx b Av b v R? = ? ?  
If we have had the SVD TU V?  of A , then we can get: 
 
Here Ty V x= , Tc U b=  
( ) ( )......(2.2)
( )
T
T T
Ax b U V x b
U V x U U b
U y c
? = ? ?
= ? ?
= ? ?
51 
 
Since U  is an orthogonal matrix, we have
2 2 2
( )Ax b U y c y c? = ? ? = ? ? , 
then the problem become finding a value of y  which can minimize 
2
y c? ?  
 
If the rank of A  is r , we have: 
1 1
0
0
0
r r
y
y
y
?
?
? ?? ?? ?? ?? ?? = ? ?? ?? ?? ?? ?? ?
?
?
,  
1 1 1
1
2
r r r
r
r
m
y c
y c
y c c
c
c
?
?
+
+
?? ?? ?? ?? ??? ?? ? = ?? ?? ?
?? ?? ?? ?
?? ?
?
?
 
So / , ( 1, 2,..., )i i iy c i r?= =  makes y c? ?  minimum as
1/ 2
2
1
m
i
i r
c
= +
? ?? ?? ?? . We 
define +?  as the transposition of ?  and inverse all the non-zero values. 
Then the first r  elements of y c+= ?  will be / , ( 1, 2,..., )i ic i r? = , and 
others are 0, and from Ty V x= , Tc U b= , we have: 
? = ?∑???? 
 
This is the minimum norm solution of LS problem. 
 
In [40] there is a more general solution: 
? = ?∑???? + ??? 
 
Vector w  can be any n r?  dimension vector. 
 
4) pseudo-inverse [41] 
 
Let TA V U+ += ? , by using LS, the solution is x A b+= . It is very similar to 
the solution 1x A b?=  of the linear function set Ax b= . So the singular value 
can be used to represent the pseudo-inverse of A  as TA V U+ += ?  [41] 
 
52 
 
7.2.3 SVD Algorithm 
 
First let us see a survey of the history of SVD. The earliest work was done by 
Beltrami in 1873. At that time lots of theory work had been done. That work is stated 
in Stewart’s paper [35]. But the numerical computation of SVD is never been 
researched until 1965, Golub and Kahan got some dramatic breakthroughs [35]. And 
designed a stable algorithm [42] (QR-iteration algorithm) in 1969. The main idea of the 
algorithm is using orthogonal transformation to transform the matrix to double 
diagonal matrix. And use double diagonal matrix iteration as the QR decomposition. 
 
In an unpublished report, Kahan proved that the singular value of the double diagonal 
matrix can be precisely calculated. Moreover, Demmel and Kahan gave a zero-shift 
QR algorithm in 1990. It has a significant relative precision for calculating the 
singular value of the double diagonal matrix [43]. Then the corresponding singular 
vector also can be precisely calculated [44]. Fernando and Parlett used qd algorithm on 
the singular value problem in 1994, and constructed a new algorithm which was faster 
and more precise for calculating the singular value than the zero-shift QR algorithm 
[45, 46]. 
 
In [47], Demmel and Veselic introduced how to apply Jacobi method to get more 
precise singular value and singular vector, but it is much slower than DK algorithm. 
And [48] did some improvement on Jacobi method and made it as fast as DK 
alforithm. 
 
There are also divide and conquer type method in [44, 45], it is can significant 
decrease the total computation cost. 
 
Bisection algorithm [41, 42] is also an effective one. 
 
Now we will state the detail of the classic QR iterationg algorithm [41, 42, and 43]. 
 
Let ( )m nA R m n?? ≥ . SVD can be calculated from the Schur decomposition of the real 
symmetric matrix TC A A=  [41]. So we can first use symmetrical QR method to 
calculate the Schur decomposition of C. But getting C will cost much time, and 
TC A A=  will cause much error. Golub and Kahan stated another stable method in 
1965, it can impliedly apply symmetrical QR method on TA A , but not calculate the 
form of TA A . 
 
The first step is to get orthogonal matrixes 1U  and 1V , such that: 
53 
 
1 1 0
n
T
m n
B
U AV
?
? ?
= ? ?? ?
 
And 
1 2
2 3
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0 0
n
n
B
? ?
? ?
?
?
? ?? ?? ?? ?= ? ?? ?? ?? ?
? ?
?
 
We can use Householder transformation to get it. Represent A as: 
[ ]1 1
1 1n
A v A
?
=  
Calculate m-rank Householder transformation 1P : 
1 1 1 1 1 1( , )
mPv e R e R? ?= ? ?  
And represent as: 
1
1 1
1
1
1
Tu
P A
mA
? ?
= ? ?
?? ??
 
Then calculate (m-1)-rank Householder transformation 1H? : 
1
1 1 2 1 2 1( , )
nH u e R e R? ? ?= ? ??  
And represent as: 
[ ]1 1 2 2
1 2n
A H v A
?
=
? ?  
Then for 2,3,..., 2k n= ? : 
a. Calculate (m-k+1)-rank Householder transformation kP? : 
1
1 1( , )
m k
k k k kP v e R e R? ? ? += ? ??  
And represent as: 
1
1
T
k
k k
k
u
P A
mA
? ?
= ? ?
?? ?
? ?  
b. Calculate (n-k)-rank Householder transformation kH? ; 
1 1 1 1( , )
n k
k k k kH u e R e R? ? ?+ += ? ??  
And represent as: 
54 
 
[ ]1 1
1 1
k k k k
n k
A H v A+ +
? ?
=
? ?  
After we get 2k n= ? , calculate (m-n+2)-rank Householder transformation 1nP ?? : 
2
1 1 1 1 1 1( , )
m n
n n n nP v e R e R? ? ? +? ? ? ?= ? ??  
And represent as: 
1 1
1
1
n
n n
n
P A
v m n
?
? ?
? ?
= ? ?
? +? ?
?  
Then calculate (m-n+1)-rank Householder transformation nP? : 
1
1 1( , )
m n
n n n nP v e R e R? ? ? += ? ??  
Now let: 
( )1, , 2,...k k kP diag I P k n?= =?  
( ), , 1,2,... 2k k kH diag I H k n= = ??  
1 1 2... nU PP P= ? 1 1 2 2... nV H H H ?=  
1 2
2 3
0 0 0
0 0 0
0 0 0
0 0 0
0 0 0 0
n
n
B
? ?
? ?
?
?
? ?? ?? ?? ?= ? ?? ?? ?? ?
? ?
?
 
Then we have: 
1 1 0
n
T
m n
B
U AV
?
? ?
= ? ?? ?
 
Till now, the first step is finished. 
 
Then we start the symmetrical QR iteration with Wilkinson shift for tridiagonal matrix
TT B B= , note that we need not calculate the whole T matrix. First get the principal 
sub matrix in the 2 2?  area of the lower right corner of TT B B=  
2 2
1 1 1
2 2
1
n n n n
n n n n
? ? ? ?
? ? ? ?
? ? ?
?
? ?+? ?
+? ?
 
Use the closest eigenvalue of 2 2n n? ?+  as the shift ? , and then determine the Givens 
transformation 1 (1,2, )G G ?= . Here cos( ), sin( )c s? ?= =  can satisfy: 
 
55 
 
 
2 2
1 1
1 2 1 2 0
Tc s c s
s c s c
?? ? ? ?
? ? ? ?
? ? ? ??
? ?? ? ? ? ? ?
= =? ? ? ?? ? ? ? ? ?
?? ? ? ? ? ?? ? ? ?
 
Here 21? ?? and 1 2? ?  are the only two non-zero elements in the positions (1, 1) and 
(1, 2) of the first row of T I??  
 
Now we determine orthogonal matrix Q  such that 1 1( )
T TQ G TG Q  is a Symmetric 
tridiagonal matrix, which means bidiagonalize 1BG . We can premultiplication and 
postmultiplication some Givens transformations, as form: 1 2 1 1 2 1... ( ) ...n n nJ J J BG G G? ? ? , 
to finish the bidiagonalization. 
 
 
Algorithm 2 
Input: ,x y  
Output: cos( ), sin( ),c s? ?= =  such that 
0
Tc s x r
s c y
? ? ? ? ? ?
=? ? ? ? ? ?
?? ? ? ? ? ?  
 
[ ]: ( , )function c s r Givens x y=  
  0if y =  
  1, 0;c s= =  
 else  
  ( )if y x>  
   2/ ; 1 , * ; 1 / ; ;x y s r y s s s c s? ? ?= ? = + = ? = =  
  else  
   2/ ; 1 , * ; 1/ ; ;y x c r x c c c s c? ? ?= ? = + = = =  
  end  
 end  
 
 
56 
 
Algorithm 3 
 
Input: ?, ?, ??, ?? 
Output: none 
 
1 2( , , , )Update c s v v  
for i = 1 to n 
1( )t v i=  
1 2( ) * * ( )v i c t s v i= ?  
2 2( ) * * ( )v i s t c v i= +  
endfor 
 
Every time it updates, multiple the singular matrixes with a Givens matrix 
c s
s c
? ?? ?
?? ?  
 
Algorithm 4 One QR iteration 
 
Input: diagonal elements ...i i? ?  and sub diagonal elements 1...i i? ?+  
Output: none 
 
( ) ( )2 2 2 21 1 / 2i i i id ? ? ? ?? ?? ?= + ? +? ? , 
2 2 2 2 2
1( ) ( )i i i id sign d d? ? ? ? ??= + + ? + , 
2
1, ,i i ix y k i? ? ? ? += ? = = , 
,Q I P I= = ;  
* 
??, ?, ?? = ??????(?, ?); 
1( , , , )k kUpdate c s q q +  
If k i>  
 k r? =  
Else 
 
57 
 
  ??, ?, ?? = ??????(?, ?); 
 k r? =  
1( , , , )k kUpdate c s p p +  
End  
If 1k i< ?  
 1
1 2 1 2
0T k
k k k k
x y c s
s c
?
? ? ? ?
+
+ + + +
? ? ? ?? ?
=? ? ? ?? ?
?? ?? ? ? ?
 
 1k k= +  
 Goto * 
Else 
T
i i
i i
c s
s c
? ?
? ?
? ? ? ?? ?
=? ? ? ?? ?
?? ?? ? ? ?
 
End 
 
 
 
Algorithm 5 Classic SVD iteration 
 
Input: ( )m nA R m n?? ≥  and error threshold ?  
Output: SVD result TA U V= ?  
 
1. Calculate Householder transformation 1, , ,nP P? ? ?  1 2, , nH H ???? , such that: 
1 1 2( ) ( ) ,0
T
n n
B n
P P A H H
m n?
? ?
? ? ? ? ? ? = ? ?
?? ?  
And 
1 2 0
0
n
n
B
? ?
?
?
? ?? ?? ?
= ? ?? ?? ?
? ?
?   
1 2: ,nU PP P= ???    
1 2 2: .nV H H H ?= ???  
 
 
 
 
58 
 
2. Set all the j to 0 which satisfy: 
( )1j j j? ? ? ? ?≤ +  
If 0, 2, ,j j n? = = ???  
  Exit algorithm; 
Else 1 : 0? = ; 
End 
Determine positive integers p q< which satisfy: 
1 0p q n? ? ?+= = ??? = = ,   0j? ≠ ? p j q< ≤  
 If there exist a i that 1p i q≤ ≤ ?  satisfy i B? ? ∞≤  
  1 1 1: 0, : , : , : 0, : 1i i i ix y l? ? ? ?+ + += = = = =  
 Else goto step 4 
 End 
 * 
         ??, ?, ?? = ??????(?, ?); 
: ,i l? ?+ =  : ( , , ) ;TU UG i i l ?= +  
If l q i< ?  
 1: i lx s? + += , 1 1:i l i lc? ?+ + + +=  
1: i ly ? + += ,  : 1l l= +  
Goto * 
 Else goto step 2 
 End 
 
3. SVD iteration: apply Algorithm 3 on: 
1
1 2
1
0
0
p p
p p
q
q
B
? ?
? ?
?
?
+
+ +
? ?? ?? ?? ?= ? ?? ?? ?? ?
? ?
?
 
Get: 
1 1:
TB P B Q= , : ( , , )p n p qU Udiag I P I ? ?= , : ( , , )p n p qV Vdiag I Q I ? ?=  
Goto step 2 
59 
 
Cost analysis: This iteration algorithm is Cubical fast convergence [42], so the time 
cost of it is just 3( )O n [43]. In [49] the time cost is about 320n  
 
7.2.4 Method comparison 
 
There are many different algorithms, but we decide to firstly finish the classical one, 
and then finish more algorithms to research their effect. From the survey of the SVD 
algorithms (some results are listed in the forms below), Classical QR algorithm 
performs stable but low accuracy and speed; zero-shift QR mix algorithm is a good 
tradeoff between the accuracy and speed. qd algorithm seems the best, but there is no 
one implementation of it. Jacobi algorithm perform the best accuracy, but not fast 
enough. Divide and conquer method perform unstable accuracy but very fast, and 
some detail of it is lacked. 
 
Table 4 includes some experiment results of DK and oqd. The values in it mean the 
increment of each cycle. 
 
 DK oqd 
cabs 2 1*2 
Divisions 2 1*2 
Multiplication 6 2*2 
Conditionals 1 0 
Assignments 7 3*2 
Auxiliary variables 6 1 
Table 4 Comparison between Dk algorithm and oqd algorithm 
 
 
Form 4 includes the experiment results of three kinds of qd algorithms. The meaning 
of values in it is the same as which in form 3. 
 
 oqd dqd qd 
cabs 1 0 0 
Divisions 2 1 1 
Multiplication 4 2 1 
Additions 1 1 1 
Subtractions 0 0 1 
Assignments 3 3 2 
Auxiliary variables 1 1 0 
Table 5 Comparison of three kinds of qd algorithms 
 
 
 
 
60 
 
8 CONCLUSION 
 
8.1 Evaluation 
 
In this report we gave a detailed description and discussion of our project. We think 
we achieve our aims of this project, and find some additional interesting topics. We 
introduced some music background and the modeling of the music structure. And we 
stated some relevant work about music information discovery and matrix 
decomposition. We researched and described many methods of the matrix 
decomposition. And we focus on one of the methods, SVD; introduce the algorithms 
of SVD and why it is useful for music structure discovery. We design some 
experiments and analyze the result of it. Then we discuss the algorithm performance 
and compare different algorithms. 
 
For the theoretical aims, we completed the comparison of different methods of matrix 
decomposition and analyzed their advantages and disadvantages. We also introduce an 
interesting topic, spectral algorithms. We deduced the equations and overview the 
applications. Combining dimensionality reduction and spectral clustering, we lead to 
SVD. We studied the mathematical meaning of SVD. We also introduced many SVD 
algorithms, and gave the basic frames. We also successfully used SVD for musical 
similarity decomposition, and compare our method with the method in the previous 
project [4]. The result shows our method is significantly better than most known 
methods for classical musical form determining. We keep the bar modeling method 
and similarity matrix building method from the previous project [4], but we compared 
different distance measures and introduce more kinds of similarity matrixes and their 
applications.  
 
For the software implementation aims, since we mainly focus on the analysis of 
method, theory and result, we just briefly describe the software details. And the basic 
frame is also from the previous project [4], we just update it and add SVD component 
into it. And we also implement some interesting additional functions include musical 
style prediction. And we redesigned the GUI to show more information about SVD 
decomposition.  
 
 
 
 
 
 
 
 
 
 
 
61 
 
8.2 Further Work 
HMM 
In the research process, we find HMM is a powerful tool for musical structure 
analysis. So we could do some more research about HMM 
 
Spectral Algorithm and manifold 
In the background research section, we find two subjects: spectral algorithm and 
manifold. They are both the hot points in machine learning area. More and more 
projects about them are created now. The normalization method and regularization 
rule are desired to research.  
 
Other Similarity Matrix 
In our project we just introduce some kinds of similarity matrix. There are more kinds 
of matrix desired to study. We should systemically collect and research their 
applications and analyze their performance. Some result may be very interesting. 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
62 
 
REFERENCE 
[1] Content-based music structure analysis NC MADDAGE - 2006 - 
scholarbank.nus.edu.sg 
 
[2] Rudiments and Theory of Music. The associated board of the royal schools of 
music, 14 Bedford Square, London, WC1B 3JG, 1949. 
 
[3] Krumhansl, C. L. (1979). The Psychological Representation of Musical Pitch 
in a Tonal Context. In Journal of Cognitive Psychology, 1979, Vol.11, No. 3, 
pp. 346-374. 
 
[4] Tom Lyner, Finding harmonic structure in music files (2009), UoB 
 
[5] Salton, G. and M. J. McGill (1983). Introduction to modern information retrieval. 
McGraw-Hill. ISBN 0070544840 
 
[6] W. Huffman, V. Pless, Fundamentals of error-correcting codes, Cambridge 
University Press, ISBN 9780521782807, 2003 
 
[7] Samet, H. (2006) Foundations of Multidimensional and Metric Data Structures. 
Morgan Kaufmann. ISBN 0123694469 
 
[8] Friedberg, Stephen H.; Insel, Arnold J.; Spence, Lawrence E. (1989), Linear 
algebra (2nd ed.), Englewood Cliffs, NJ 07632: Prentice Hall, ISBN 0-13-537102-3 
 
[9] "Vorbis I Specification". Xiph.org. 2007-03-09. Retrieved 2007-03-09. 
 
[10] Ronald Fisher (1936)The Use of Multiple Measurements in Taxonomic Problems 
In: Annals of Eugenics, 7, p. 179—188 
 
[11] McLachlan (2004)Discriminant Analysis and Statistical Pattern Recognition In: 
Wiley Interscience 
 
[12] Pattern recognition and machine learning CM Bishop - 2006 - library.wisc.edu 
 
[13] Nicholas J. Higham, Recent Developments in Dense Numerical Linear Algebra 
(2000), http://citeseer.ist.psu.edu/higham00recent.html 
 
[14] Bae, D-S; Haug EJ (1988). "A Recursive Formulation for Constrained 
Mechanical System Dynamics: Part I. Open Loop Systems". Mechanics of Structures 
and Machines 15: 359–382 
 
63 
 
[15] Shaw PJA (2003) Multivariate statistics for the Environmental Sciences, 
Hodder-Arnold. 
 
[16] Ravi Kannan and Adrian Vetta, On clusterings: good, bad and spectral. Jour 
nal of the ACM (JACM) 51(3), 497--515, 2004. 
 
[17] A.Y. Ng, M.I. Jordan, and Y. Weiss. On spectral clustering: Analysis and a 
n algorithm. Proc. Neural Info. Processing Systems (NIPS 2001), 2001. 
 
[18] H. Zha, C. Ding, M. Gu, X. He, and H.D. Simon. Spectral relaxation for K-m 
eans clustering. Advances in Neural Information Processing Systems 14 (NIPS 20 
01). pp. 1057-1064, Vancouver, Canada. Dec. 2001. 
 
[19] Deng Cai, Xiaofei He, Jiawei Han, "Document Clustering Using Locality 
Preserving Indexing," IEEE Transactions on Knowledge and Data Engineering, vol. 
17, no. 12, pp. 1624-1637, Dec. 2005, doi:10.1109/TKDE.2005.198 
 
[20] J. Shi and J. Malik. Normalized cuts and image segmentation. IEEE. Trans.  
on Pattern Analysis and Machine Intelligence, 22:888--905, 2000. 
 
[21] M. Belkin and P. Niyogi. Laplacian Eigenmaps and Spectral Techniques for E 
mbedding and Clustering, Advances in Neural Information Processing Systems 14  
(NIPS 2001), pp: 585-591, MIT Press, Cambridge, 2002. 
 
[22] John A. Lee, Michel Verleysen, Nonlinear Dimensionality Reduction, Springer, 
2007 
 
[23] Laplacian Eigenmaps for Dimensionality Reduction and Data Representation  
Mikhail Belkin, Partha Niyogi, Neural Computation June 2003, Vol. 15, No. 6, Pages 
1373-1396: 1373-1396. 
 
[24] Spectral Regression: A Unified Approach for Sparse Subspace Learning, Deng 
Cai;   Xiaofei He;   Jiawei Han; Data Mining, 2007. ICDM 2007. Seventh IEEE 
International Conference on 
 
[25] F.R.K. Chung. Spectral Graph Theory. Amer. Math. Society Press, 1997. 
 
[26] M. Meila and J. Shi. A random walks view of spectral segmentation. Int'l W 
orkshop on AI & Stat (AI-STAT 2001) 
 
[27] Jeff Cheeger, A lower bound for the smallest eigenvalue of the Laplacian. 
Problems in analysis (Papers dedicated to Salomon Bochner, 1969), pp. 195–199. 
Princeton Univ. Press, Princeton, N. J., 1970 MR0402831 
 
64 
 
[28] A Divide-and-Merge Methodology for Clustering (D. Cheng, R. Kannan and G.  
Wang) Proc. of the ACM Symposium on Principles of Database Systems, 2005. 
 
[29] Jonathan T. Foote, "Content-based retrieval of music and audio", Proc. SPIE 
3229, 138 (1997) 
 
[30] Jonathan T. Foote,  An overview of audio information retrieval, MULTIMEDIA 
SYSTEMS Volume 7, Number 1, 2-10 
 
[31] J. Foote and M. Cooper. Media segmentation using self-similarity decomposition. 
In Proc. SPIE Storage and Retrieval for Media Databases, volume 5021, pages 
167-175, Santa Clara, California, USA, 2003. 
 
[32] P. A. Businger and G. H. Golub, Algorithm 358: Singular value decomposition of 
a complex matrix, Assoc. Comp. Math, 12:564-565, 1969. 
 
[33] S. Dubnov and T. Appel, “Audio segmentation by singular value clustering,” in 
Proc. of Int.Conf. on Computer Music (ICMC), 2004. 
 
[34] Steen, Lynn Arthur; Seebach, J. Arthur Jr. (1995) [1978], Counterexamples in 
Topology, Dover, ISBN 978-0-486-68735-3, OCLC 32311847 
 
[35] G. W. Stewart, On the early history of the singular value decomposition, SIAM 
Rev., 35:551-566,1993. 
 
[36] G. H. Golub and W. Kahan, Caculating the singular values and pseudo-inverse of 
a matrix, SIAM J. Numer. Anal., 2:205-224, 1965 
 
[37] P. A. Businger and G. H. Golub, Algorithm 358: Singular value decomposition of 
a complex matrix, Assoc. Comp. Math, 12:564-565, 1969. 
  
[38] Nicholas J. Higham, Recent Developments in Dense Numerical Linear Algebra 
(2000), http://citeseer.ist.psu.edu/higham00recent.html 
 
[39] James W. Demmel and W. Kahan, Accurate singular values of bidiagoanl 
matrices, SIAM J. Sci. Stat. Comput, 11(5):873-912, 1990. 
 
[40] Percy Deift, James W. Demmel, Luen-Chau Li, and Carlos Tomei, The 
bidiagonal singular value decomposition and Hamiltonian mechanics, SIAM J. Numer. 
Anal, 28:1463-1516, 1991. 
 
[41] K. Vince Fernando and Beresford N. Parlett, Accurate singular values and 
differential qd algorithms, Numer. Math, 67:191-229, 1994. 
 
65 
 
[42] Beresford N. Parlett, The new qd algorithms, In Acta Numerica, Cambridge 
University Press, 1995, pages 459-491. 
 
[43] Demmel James and Veselic Kresimir, Jacobi's method is more accurate than QR, 
SIAM J. Matrix Anal. Appl. 13 (1992), no. 4, 1204--1245. 
 
[44] Z.DRMAC, A posteriori computation of the singular vectors in a preconditioned 
Jacobi SVD algorithm, IMA J. Numer. Anal., 19(1999), pp191-213. 
 
[45] Ming Gu and Stanley C. Eisenstat, A divide-and-conquer algorithm for the 
bidiagonal SVD, SIAM Journal on Matrix Analysis and Applications Volume 16, 
Number 1 pp. 79-92, 1995. 
 
[46] I. Dhillon, J. Demmel, M. Gu, Efficient Computation of the Singular Value 
Decomposition with Applications to Least Squares Problems, LBL Report #36201, 
1994, TECHNICAL REPORT. 
 
[47] J. Barlow and J. Demmel, Computing accurate eigensystems of scaled diagonally 
dominant matrices, SIAM J. Numer. Anal., 27(1990), pp762-791 
 
[48] K. V. Fernando and B. N. Parlett, Accurately counting singular values of 
bidiagonal matrices and eigenvalues of skew-symmetric tridiagonal matrices, SIAM J. 
Matrix Anal. Appl., 20(1998),pp373-399 
 
[49] Jesse L. Barlow, More Accurate Bidiagonal Reduction for Computing the 
Singular Value Decomposition, SIAM Journal on Matrix Analysis and Applications, 
Volume 23, Number 3 pp. 761-798, 2002. 
 
[50] E. Anderson, Z. Bai, C. H. Bischof, S. Blackford, J. W. Demmel, J. J. Dongarra, J. 
Du Croz, A. Greenbaum, S. Hammarling, A. McKenney, and D. C. Sorensen, Lapack 
Users' Guide, 3rd edn. SIAM Press, Philadelphia PA, 1999. 
 
 
 
 
 
 
 
66 
 
APPENDICES 
Appendix A: Laplacian Eigenmap 
% Laplacian Eigenmap ALGORITHM (using K nearest neighbors) 
% 
% [Y] = le(X,K,dmax) 
% 
% X = data as D x N matrix (D = dimensionality, N = #points) 
% K = number of neighbors 
% dmax = max embedding dimensionality 
% Y = embedding as dmax x N matrix 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
  
function [Y] = leigs(X,K,d) 
  
[D,N] = size(X); 
fprintf(1,'LE running on %d points in %d dimensions\n',N,D); 
  
  
% STEP1: COMPUTE PAIRWISE DISTANCES & FIND NEIGHBORS  
fprintf(1,'-->Finding %d nearest neighbours.\n',K); 
  
X2 = sum(X.^2,1); 
distance = repmat(X2,N,1)+repmat(X2',1,N)-2*X'*X; 
  
[sorted,index] = sort(distance); 
neighborhood = index(2:(1+K),:); 
  
  
  
% STEP2: Construct similarity matrix W 
fprintf(1,'-->Constructing similarity matrix.\n'); 
  
W = zeros(N, N); 
for ii=1:N 
    W(ii, neighborhood(:, ii)) = 1; 
    W(neighborhood(:, ii), ii) = 1; 
end 
  
% STEP 3: COMPUTE EMBEDDING FROM EIGENVECTS OF L 
fprintf(1,'-->Computing embedding.\n'); 
  
D = diag(sum(W)); 
67 
 
L = D-W; 
  
% CALCULATION OF EMBEDDING 
options.disp = 0; options.isreal = 1; options.issym = 1;  
[Y,eigenvals] = eigs(L,d+1,0,options); 
Y = Y(:,2:d+1)'*sqrt(N); % bottom evect is [1,1,1,1...] with eval 0 
  
  
fprintf(1,'Done.\n'); 
  
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
